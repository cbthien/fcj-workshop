[{"uri":"https://cbthien.github.io/fcj-workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Cao Bá Thiên\nPhone Number: 0961161479\nEmail: thiencbse184423@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 06/09/2025 to 9/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This worklog covers 12 weeks of internship at First Cloud Journey, focusing on building ARC-Chatbot — a RAG Chatbot application on AWS.\nWeek 1: AWS Account \u0026amp; IAM Fundamentals\nWeek 2: AWS Budgets \u0026amp; Support Essentials\nWeek 3: IAM Advanced Concepts \u0026amp; Core VPC Networking\nWeek 4: EC2 Operations \u0026amp; Application Deployment\nWeek 5: Cloud9, S3 \u0026amp; Static Website Architecture\nWeek 6: RDS MySQL \u0026amp; Lightsail Deployment\nWeek 7: Real-World AWS Architecture \u0026amp; Text-to-SQL Project\nWeek 8: Midterm Review \u0026amp; Exam Preparation\nWeek 9: Auto Scaling, Load Balancing \u0026amp; CloudWatch Monitoring\nWeek 10: AWS CLI, DNS, Hybrid Networking \u0026amp; Microsoft AD\nWeek 11: Base Infrastructure (M0) \u0026amp; IDP Pipeline (M1)\nWeek 12: RAG Chat Component (M2) \u0026amp; Testing/Golive (M3)\n"},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand the structure of an AWS account and the role of the Root User. Learn how to create and secure an AWS account. Set up IAM Users, IAM Groups and attach permission policies. Enable MFA and configure cost alerts. Get familiar with the AWS Management Console interface. Tasks carried out during the week: Day Tasks Start Date Completion Date Reference 1 - Overview of the AWS account and responsibilities of the Root User - Understand IAM concepts (User, Group, Policy) 08/09/2025 08/09/2025 https://000001.awsstudygroup.com/ 2 - Create an AWS account - Add a payment method - Verify email \u0026amp; phone number - First login and explore the AWS Console 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/ 3 - Secure the Root account + Enable MFA + Configure password policy + Minimize Root User usage - Configure Billing Preferences \u0026amp; monitor Free Tier 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/ 4 - Create IAM Group (Administrators) - Attach AdministratorAccess policy - Create IAM User - Configure sign-in for the user and enable MFA 11/09/2025 11/09/2025 https://000001.awsstudygroup.com/ 5 - Create Budget and Billing Alerts - Review the account security checklist - Practice logging in using IAM User - Explore the AWS Console interface - Summarize lessons learned \u0026amp; issues encountered 12/09/2025 12/09/2025 https://000001.awsstudygroup.com/ Week 1 Achievements Week 1 was the phase where I “laid the foundation” for my entire journey with AWS, so I tried to do everything carefully and systematically:\nMastering the AWS account structure\nI understood clearly the difference between:\nRoot User – only used for high-level, sensitive administrative tasks. IAM User / IAM Group / Policy – used for day-to-day access management following best practices.\nThanks to that, I no longer thought in a simplistic way like “just log in with root and it’s done”, but became aware of long-term security considerations. Creating and securing the AWS account in a structured way\nI did not stop at just creating an AWS account, but also:\nSet up full payment information, verified email and phone number. Double-checked each step to ensure the account was operating stably.\nThis helped me feel more confident when using the account for labs and later projects. Protecting the Root account and IAM Users according to best practices\nEnabled MFA for the Root User and for the main IAM User that I use daily. Configured a password policy with strict rules (length, special characters, password lifetime, etc.). Minimized logging in as Root and used only IAM Users for operations.\nThrough this, I realized the importance of security from the very beginning, instead of only caring about “making the service run”. Setting up cost management and early alerts\nConfigured Billing Preferences to receive emails when there are changes. Created Budgets and Billing Alerts to avoid exceeding the Free Tier.\nThanks to these steps, I felt more at ease when doing hands-on exercises because I knew that if any unusual costs occurred, I would be notified early. Applying IAM based on best practices instead of doing it superficially\nCreated an IAM Group (Administrators) and attached AdministratorAccess properly. Created a separate IAM User and added it to the group instead of attaching policies directly to the user. Tried logging in with the IAM User, checked permissions and tested enabling MFA.\nThese actions helped me understand that IAM is not just theory, but an important tool for operating a secure system. Getting familiar with the AWS Management Console and service-oriented thinking\nSpent time exploring the interface, searching for services through the search bar. Took notes of services that appeared often in documentation (EC2, S3, IAM, CloudWatch, etc.).\nAfter Week 1, the initial feeling of being “overwhelmed” by the AWS interface decreased significantly, replaced by familiarity and a clearer picture when reading documentation/architecture. Self-assessment and difficulties encountered in Week 1 Although Week 1 did not involve much “building systems”, it was the week where I had to change my mindset about using a cloud account in a professional manner.\n1. Self-assessment Being proactive and seeing tasks through step by step\nI didn’t just complete the checklist, but tried to understand “why this step is necessary”, especially the parts related to Root User, IAM and Billing. When there was something unclear, I actively looked up documentation, asked my mentor, and took notes.\nFollowing processes and focusing on security from the start\nInstead of skipping parts such as MFA or Budget (which are often underestimated), I seriously completed them all. This helped me form good habits for later weeks, when the system becomes more complex.\nAble to structure and systematize knowledge\nAt the end of each day, I summarized: what I learned today, which steps were completed, what I was still stuck on. This habit helped me avoid “losing track” and allowed me to review quickly when needed.\n2. Difficulties encountered Confusion with account \u0026amp; permission concepts\nAt first, I was quite confused in distinguishing:\nWhen to use Root, when to use IAM User? What is the difference between assigning permissions to a Group vs. assigning directly to a User?\nAfter reading more AWS documentation and trying several times, I understood the permission model more clearly. Concerns about costs and billing\nBecause I had to add a payment method (card/bank), I was quite worried about being charged unexpectedly. This made me more cautious, but also cost me extra time to:\nRead the Billing \u0026amp; Free Tier sections carefully. Learn how to create Budgets and Alerts.\nIn the long run, this is a “good difficulty”, because it helped me understand cost control more deeply. Too much new information in a short time\nRight from the first week I had to get used to:\nConcepts around accounts, security, and billing. The Console interface with many services.\nSometimes I felt a bit overwhelmed, but I handled it by: Breaking down the content by day (as in the task table). Focusing only on what was directly related to account \u0026amp; security in Week 1. Overall, although Week 1 was mainly about “laying the groundwork”, I tried to do everything carefully and thoughtfully instead of just doing it for the sake of completion. This is an important foundation so that in Week 2 and Week 3 I can focus more on services and architecture without having to go back and fix basic issues related to the account and security.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/3-blogstranslated/3.1-blog1/","title":"FotMob delivers near real-time football updates to millions of fans with AWS","tags":[],"description":"","content":"by Emily McKinzie | on 25 MAR 2025 | in Amazon Aurora, Amazon Bedrock, Amazon CloudFront, Amazon EC2,Amazon Elastic Kubernetes Service, Amazon ElastiCache, Amazon Machine Learning, Amazon Simple Storage Service (S3), Artificial Intelligence, AWS Lambdak,Compute, Database, Game Development, Games, Generative AI, Industries, Networking \u0026amp; Content Delivery, Storage\nThe world’s most-watched sport, professional football (soccer) attracts a global fanbase of five billion people. With hundreds of thousands of players participating in matches across the world, the sport also generates staggering amounts of data, from goals to saves, assists, and beyond. In today’s connected world, fans want real-time access to all the match, player, and team stats, driving demand for matchday apps like FotMob. Home to over 17 million users, FotMob delivers live football updates and news spanning 500 international leagues nearly all day, every day.\nWith its infrastructure built on Amazon Web Services (AWS), the Norwegian company behind the app can scale and innovate to continuously elevate the fan experience. Recently, the team developed an in-house push notification system that not only enhances near real-time updates for fans but also saves over $130,000 USD annually.\nThese savings will be paramount to developing new engaging app features, as the organization aims to double its user base in the coming years. Recognizing personalization will be key to this growth strategy, the FotMob team is also experimenting with generative AI-powered team summaries built with Amazon Bedrock.\nFrom humble beginnings to top-rated matchday app FotMob started out in 2004 as a passion project led by founders and brothers Christer and Tommy Nordvik. The two worked evenings and weekends, juggling full-time jobs and family responsibilities, to build the company from the ground up. However, it wasn’t until 2008, when smartphones began popularizing, that FotMob took on a life of its own. Since then, the team has extended its offering to include more in-depth match statistics, personalized notifications, and team and match news summaries spanning multiple regions.\n“We built the app to cater to all user types, whether the casual fan who wants to check in on scores or the more passionate ones who want to delve deep into the data. Using machine learning, we’ve been able to transform stats from data providers, like Stats Perform, into easy-to-digest visuals, like a momentum graph or player rating system,” explained FotMob Founder Christer Nordvik. “The latter of the two gives users a better understanding of a player’s skillset; some scouts even use it as a player scouting tool.”\nRevamping the push notification system to scale with an expanding user base When FotMob began sending push notifications to users, it originally enlisted third-party providers. However, it quickly found they couldn’t support the volume of notifications the app needed to distribute the updates and crashed during peak traffic periods. Pivoting, Christer and Tommy quickly began plans to develop a solution in-house and looked to AWS for an assist.\n“Our fans are loyal, but FotMob is a live sports service, so there’s an expectation our app will be stable; downtime isn’t an option,” Christer explained. “If something breaks or we’re dark for more than a few minutes, users will go elsewhere. We knew AWS could give us that peace of mind, and we were right; AWS provides incredible stability.”\nOn an average week, FotMob’s system may send over three billion push notifications, and in just one day, it can see five billion HTTP requests each hour, which translates to 1.6 million requests every second. With its infrastructure now powered by AWS, FotMob can support near-instant push notification delivery across regions, even during peak traffic periods. Amazon Simple Storage Service (Amazon S3) and Amazon Aurora enable the team to store match data and highlights in a multi-region setup. Amazon Elastic Cloud Compute (Amazon EC2) processes and distributes the match updates in near real-time, while Amazon Elastic Kubernetes Service (Amazon EKS) and Amazon ElastiCache support the push notification delivery.\nFotMob also uses Amazon CloudFront to confirm rapid delivery of other static content to users, AWS Lambda for scalable application programming interfaces (APIs), and AWS Cloud Development Kit (AWS CDK) for infrastructure as code. Christer added, “With AWS as our backend, we don’t get hung up on infrastructure challenges, so we can innovate faster to deliver features fans care about. They get quick, reliable updates without any disruption.”\nCharting the course ahead With a vision to expand its user base to 30 million, FotMob understands the importance of further personalizing the user experience. It recently began experimenting with generative AI using Amazon Bedrock to help create more tailored news and team summaries, with match reports designed to engage fans.\nThus far, FotMob has tested several large language models, with Anthropic’s Claude proving the most cost-efficient and delivering optimal results. Using the model, FotMob successfully launched a team summary feature for its app in English and is already planning to use generative AI to support additional languages.\n“Amazon Bedrock gives us a scalable and efficient framework to deliver the AI-generated summaries fans want,” Christer noted. “It integrates with our existing AWS services and lets us quickly test different models to ensure accuracy and minimize costs.”\nHe believes FotMob’s continued emphasis on user experience and innovation sets the company apart in the sports app landscape and that AWS is integral to maintaining this edge. Christer concluded, “One of the greatest benefits with AWS is having everything we need from a single provider. Even when we’re exploring new tech, we often go to AWS first to see if they have something that suits, as it’s easier to have everything in one place.”\nDownload FotMob to see AWS technology in action and experience near real-time football updates. Explore how you can build, run, and grow applications with AWS or connect with an AWS for Games team member for more information.\nRecommended reading - Bundesliga powered by AWS - NFL on AWS\n- How DAZN uses AWS Step Functions to orchestrate event-based video streaming at scale\n- Hudl Scales Video Processing and Boosts Reliability by Optimizing on Amazon EC2 Spot Instances Source:\nhttps://aws.amazon.com/blogs/gametech/fotmob-delivers-near-real-time-football-updates-to-millions-of-fans-with-aws/\n"},{"uri":"https://cbthien.github.io/fcj-workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Reflection Report “AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS” Event Objectives Introduce the AI/ML/GenAI ecosystem on AWS and how it can be applied to real-world use cases. Guide participants through end-to-end ML deployment with Amazon SageMaker. Build Generative AI applications using Amazon Bedrock, RAG, Agents, and AgentCore. Get familiar with MLOps, IaC, CICD, and container workflows for AI/ML workloads. Develop operational thinking and AI architecture design in enterprise environments. Speakers Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Van Hoang Kha – Community Builder Key Highlights AWS AI/ML Services Overview Amazon SageMaker — end-to-end ML platform Data preparation:\nSageMaker Data Wrangler Ground Truth (data labeling) Feature Store (managing reusable features) Training \u0026amp; Tuning:\nTraining jobs with GPU/CPU autoscaling Supports distributed training Hyperparameter tuning (Auto-tuning, Bayesian Optimization) Deployment Options:\nReal-time endpoint Serverless inference Multi-model endpoint Asynchronous (async) inference MLOps:\nModel registry CI/CD pipelines with SageMaker Pipelines Monitoring drift: data drift, feature drift, model drift Live demo:\nHands-on with SageMaker Studio: notebooks, data processing, training, and model deployment. Generative AI with Amazon Bedrock Foundation Models (FMs) AWS provides a fully managed library of foundation models from leading providers (Anthropic, Meta, Amazon, etc.):\nClaude 3.5 – strong reasoning, long context, great for coding. Llama 3 – open-weight, easy to fine-tune, cost-effective. Titan – high-quality embeddings for RAG. Mistral – fast, lightweight, suitable for realtime applications. → Enables quick customization and integration without having to train models from scratch.\nPrompt Engineering Techniques Understanding how to “guide” the model through various prompting strategies:\nZero-shot – the model only receives the task description. Few-shot – the model is given a few example inputs/outputs. Chain-of-Thought – ask the model to show step-by-step reasoning for higher accuracy. Role prompting – assign a specific role to the model. Multi-step reasoning \u0026amp; Prompt templates – break down reasoning and standardize format. Retrieval Augmented Generation (RAG) Improving answer quality by injecting external knowledge:\nR – Retrieval: fetch relevant information from a knowledge base / data store. A – Augmentation: include the retrieved information in the prompt as context. G – Generation: the model produces answers that are more accurate and well-grounded. Use cases:\nContext-aware enterprise chatbots Intelligent search (contextual / semantic search) Summarizing documents, logs, and real-time data Amazon Titan Embeddings Lightweight embedding model that converts text into dense vectors. Used for similarity search, semantic search, and RAG workflows. Multilingual support, optimized for enterprise knowledge use cases. AWS AI Services – Pre-built AI APIs Rekognition – image/video analysis Translate – language translation Textract – extract text \u0026amp; layout from documents Transcribe – speech-to-text Polly – text-to-speech Comprehend – natural language processing Kendra – intelligent enterprise search Lookout – anomaly detection Personalize – personalized recommendations Highlight demo:\nThe AMZPhoto face recognition app, showcasing how to integrate AI/ML into real-world products on AWS.\nBedrock Agents \u0026amp; AgentCore Bedrock Agents Agents can autonomously execute multi-step workflows. Trigger Lambda to call APIs, databases, and external workflows. Can replace parts of backend business logic in many use cases. Integrates tightly with EventBridge and Step Functions for orchestration and retry within AI pipelines. Amazon Bedrock AgentCore A new framework for building production-ready AI Agents:\nExecute and scale agent workflows securely. Manage long-term memory. Provide detailed identity \u0026amp; access control. Integrate with tools such as Browser Tool, Code Interpreter, Memory Store. Provide observability \u0026amp; auditing capabilities. Support many popular agent frameworks: CrewAI, LangGraph, LlamaIndex, OpenAI Agents SDK, etc. CICD Workflow for Containers (ECR + ECS) A standard AWS DevSecOps pipeline for AI/ML containers:\nDeveloper commits code → CodeCommit CodeBuild builds the image and runs tests Push image to ECR ECS pulls the image to deploy (Fargate / EC2) CloudWatch monitors logs \u0026amp; metrics CodePipeline orchestrates the entire process DevSecOps (security in the pipeline):\nValidation in CodeBuild (unit tests, static analysis) Image scanning in ECR (vulnerability scan) Deployment policies / IAM controls to manage deployment → This represents AWS-standard DevSecOps.\nWhat I Learned Core AI/ML \u0026amp; GenAI Knowledge A clear view of the ML lifecycle. Understanding of feature engineering, model deployment \u0026amp; monitoring. How to optimize training/inference costs. How to choose the right Foundation Model for each use case. Advanced prompt engineering techniques (CoT, role, multi-step, RAG-aware prompting). How to design RAG architectures that are production-ready. How to build Agents that can automatically execute complex workflows. Key Technical Architecture IaC (Infrastructure as Code): Terraform vs CloudFormation vs CDK. Container inference workflow: ECS / ECR for AI inference containers. Monitoring \u0026amp; Observability: monitoring AI workloads with CloudWatch + X-Ray. Applying to Work Build enterprise chatbots with Bedrock + RAG (retrieval-augmented generation). Design end-to-end AI architectures (data → ML → app). Use Lambda + Step Functions to orchestrate RAG pipelines. Use Terraform / CDK to build enterprise-grade GenAI infrastructure. Deploy inference containers using ECS / Fargate. Apply knowledge of AgentCore and Agents to future internal GenAI projects. Event Experience Attending the “AWS Cloud Mastery Series #1” workshop helped me clearly see how AWS deploys AI/ML in real-world scenarios, understand the differences between traditional ML and GenAI, and improve my AI architecture design mindset.\nI learned from highly experienced speakers and gained a detailed understanding of how AWS builds a comprehensive AI/ML/GenAI ecosystem.\nHands-on with modern tools:\nSageMaker: from data preparation → training → tuning → deployment → MLOps. Bedrock: using Claude, Llama, Titan, RAG + Agents to quickly build chatbots and AI workflows. IaC \u0026amp; DevOps: CloudFormation, CDK, Terraform, CodePipeline for controlled and automated AI/ML deployments. Data \u0026amp; ETL Tools: Glue, S3 (Data Lake), EventBridge, Step Functions for data pipelines. Monitoring: CloudWatch for logging \u0026amp; drift detection. On networking and mindset:\nLearned to think in terms of AI systems, not just standalone ML models. Gained insights into AI/ML trends in Vietnam and how enterprises are adopting them. Key Lessons GenAI and traditional ML complement each other to create powerful real-world AI solutions. Bedrock is an enterprise-grade GenAI platform: secure, fast to deploy, no need to train models from scratch. MLOps + IaC are essential for keeping AI systems stable in production. RAG is one of the most effective ways to “bring enterprise knowledge into the model”. Overall, the event not only provided technical knowledge but also helped me build a complete AI/ML system design mindset, from data → model → deployment → operations, forming a solid foundation for future real-world AI/GenAI projects.\nSome photos from the event "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.1-introduction/","title":"Introduction","tags":[],"description":"","content":"Problem Statement Traditional chatbot systems struggle without the ability to access specific information from internal documents, leading to inaccurate or irrelevant responses. This workshop solves this problem by building an architecture capable of:\nAutomation: Automatically process and index PDF documents Content Inquiry: Receive queries and guide users to relevant content Document Retrieval: Answer complex questions with accurate source citations from documents Solution Architecture The system is designed following the RAG (Retrieval-Augmented Generation) model combined with AWS Serverless to ensure scalability:\nFrontend Interface: Users interact via React Web Application\nAmazon API Gateway receives requests from Frontend AWS Amplify hosting with CloudFront CDN Amazon Cognito handles authentication Request Handling:\nApplication Load Balancer routes traffic to EC2 FastAPI Backend processes REST API requests Amazon SQS (FIFO) ensures document processing order Backend Processing:\nChatHandler: Manages conversations, saves sessions to Amazon DynamoDB RAG Service: Orchestrates vector search and LLM generation Qdrant Vector Database: Self-hosted on EC2 for vector search AI \u0026amp; Data Layer:\nAmazon Bedrock: Uses Claude 3.5 Sonnet (LLM) and Cohere Embed Multilingual v3 (Embeddings) Amazon Textract: OCR and text extraction from PDF Amazon S3: Document storage Amazon DynamoDB: Metadata and chat history Admin Dashboard:\nReact-based interface hosted on AWS Amplify Upload and manage documents Monitor processing status View chat history Architecture Key Technologies In this workshop, you will work with the following key AWS services:\nAmazon Bedrock: The heart of AI, providing Foundation Models (Claude, Cohere) for language processing and embedding generation Amazon Textract: Build IDP pipeline to extract text from PDF documents Amazon EC2 \u0026amp; VPC: Compute and network infrastructure for backend services Amazon S3: Document and static asset storage Amazon DynamoDB: Store metadata, chat history, and document status Amazon Cognito: Authentication and user management AWS Amplify: Frontend application hosting with integrated CI/CD Amazon SQS: Message queue for document processing pipeline Qdrant (Self-hosted): Vector database for semantic search Terraform (IaC): Deploy entire infrastructure as code "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand AWS Budgets and how to use them to manage and monitor AWS costs. Learn different types of AWS Budgets: Cost Budget, Usage Budget, RI Budget, Savings Plans Budget. Practice creating budgets using templates and custom settings, and cleaning up resources. Understand AWS Support: support plans, how to access AWS Support, and how to create and manage support requests. Build awareness of cost control and how to get help from AWS when issues occur. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study AWS Budgets overview + What is AWS Budgets? + Why use budgets to control costs? + Types of budgets (Cost, Usage, RI, Savings Plans) 15/09/2025 15/09/2025 https://000007.awsstudygroup.com/ 2 - Hands-on with Budgets (1): + Create Budget by Template + Create a Cost Budget with alert thresholds + Review budget details and notification settings 16/09/2025 16/09/2025 https://000007.awsstudygroup.com/ 3 - Hands-on with Budgets (2): + Create a Usage Budget for a specific service (e.g., EC2) + Create RI Budget + Understand when to use each type of budget 17/09/2025 17/09/2025 https://000007.awsstudygroup.com/ 4 - Hands-on with Budgets (3): + Create Savings Plans Budget + Review alerts and examples of cost overrun scenarios + Clean up budgets and related resources after the lab 18/09/2025 18/09/2025 https://000007.awsstudygroup.com/ 5 - Study AWS Support: + AWS Support Plans and their differences + How to access AWS Support Center - Hands-on: + Navigate to Support Center + Create a support case + View / update / close support requests 19/09/2025 19/09/2025 https://000009.awsstudygroup.com/ Week 2 Achievements Week 2 focused on building a solid foundation in cost management and support processes on AWS. Instead of only learning services, I learned how to keep an AWS environment sustainable and under control.\nUnderstanding the role of AWS Budgets in cost management\nI gained a clear understanding of:\nWhat AWS Budgets is and how it differs from just looking at the Billing dashboard. Why proactive budget configuration is important to avoid unexpected cost spikes. How budgets fit into an overall cost management strategy for workloads on AWS. Differentiating between multiple budget types\nI learned to distinguish the use cases and strengths of each budget type:\nCost Budget – controls total spending (e.g., all services in a month). Usage Budget – tracks consumption of specific resources (e.g., EC2 hours, S3 storage). RI Budget – monitors Reserved Instances utilization and coverage. Savings Plans Budget – tracks Savings Plans spend and helps ensure commitments are used effectively.\nThis helped me understand that “one budget type does not fit all” and each workload may require a different combination. Hands-on experience creating and configuring budgets\nI didn’t just read documentation – I practiced:\nCreating budgets using templates for faster setup. Creating custom Cost Budgets with specific thresholds and time periods. Reviewing detailed budget configuration, including scope, period, and filters.\nThrough this, I became more confident working inside the Billing \u0026amp; Cost Management console rather than being hesitant to touch it. Setting up budget alerts and notification channels\nI configured email alerts so that:\nNotifications are sent when actual costs exceed a defined threshold. Notifications are also sent when forecasted costs are expected to exceed the budget.\nThis gave me a realistic view of how budgets can be used in real environments to catch issues before they turn into actual overspending. Cleaning up budgets and remaining resources after the lab\nAfter finishing the hands-on work:\nI reviewed existing budgets and deleted those no longer needed. I double-checked that no unnecessary alerts or test configurations remained.\nThis reinforced the habit of cleaning up after experiments to keep the account tidy and easier to manage. Understanding AWS Support and when to use it\nI learned:\nThe differences between various AWS Support Plans (Basic, Developer, Business, Enterprise). Which features are available under the Basic plan (e.g., documentation, forums) and which require paid plans. Realistic scenarios where opening an AWS support case is the right approach instead of trying to fix everything alone. Hands-on practice with AWS Support Center\nI practiced:\nNavigating to the AWS Support Center from the console. Creating a support case, choosing the right category and severity level. Viewing, updating, and closing support requests.\nThis helped demystify the support process and made me more comfortable asking for help in a structured way when needed. Building stronger awareness of cost control and support processes\nOverall, Week 2 helped me realize that:\nUsing AWS responsibly is not only about deploying resources, but also about actively monitoring costs. Knowing how and when to contact AWS Support is an important skill, especially when working in production-like environments.\nThis mindset will be very valuable as future weeks move on to more complex architectures and services. 1. Self-assessment More confident working with the Billing console\nAt the beginning, I was still a bit hesitant to touch the Billing page due to fear of misconfiguring something. By the end of the week, after creating, editing, and deleting multiple budgets, I became more comfortable navigating and understanding cost-related information.\nImproved understanding of cost visibility and alerts\nI started to think in terms of:\n“What will happen if I forget to stop some resources?” “How can I know in advance if my costs are going out of control?”\nThis is a big improvement compared to only thinking about technical functionality. Better structured approach to learning\nI followed a clear sequence:\nLearn concepts → do hands-on labs → review results → clean up resources.\nThis helped me retain knowledge longer and avoid just “clicking around” without understanding. 2. Challenges encountered Understanding when to use each type of budget\nAt first, I found it a bit confusing:\nWhen should I use a Cost Budget vs. a Usage Budget? In what scenarios are RI and Savings Plans budgets really necessary?\nI overcame this by mapping each budget type to realistic usage scenarios (e.g., long-term EC2 usage, committed usage plans, or student/free-tier environments). Interpreting forecast vs. actual metrics\nIt took some time to understand:\nThe difference between actual and forecasted cost/usage. Why alerts can be triggered even when current spending is still below the budget (due to forecast).\nThis required careful reading of the console and experimenting with different thresholds. Getting used to the support case creation flow\nWhen first creating a support case, I struggled a bit with:\nChoosing the correct category and severity level. Writing a clear and concise problem description.\nWith practice, I learned how to describe issues more effectively, which is an important skill in real projects. Overall, Week 2 strengthened my understanding of cost management and support workflows on AWS. These skills are essential for operating real workloads in the cloud, not just for passing labs or tutorials. The knowledge I gained will help me avoid common pitfalls like unexpected bills and feeling “stuck” when something breaks in a cloud environment.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/3-blogstranslated/3.2-blog2/","title":"Accelerating the next wave of generative AI startups","tags":[],"description":"","content":"by Swami Sivasubramanian | on 13 JUN 2024 | in AWS for Startups, Featured, Startup, Startup Spotlight\nSince day one, AWS has helped startups bring their ideas to life by democratizing access to the technology powering some of the largest enterprises around the world including Amazon. Each year since 2020, we have provided startups nearly $1 billion in AWS Promotional Credits. It’s no coincidence then that 80% of the world’s unicorns use AWS. I am lucky to have had a front row seat to the development of so many of these startups over my time at AWS—companies like Netflix, Wiz, and Airtasker. And I’m enthusiastic about the rapid pace at which startups are adopting generative artificial intelligence (AI) and how this technology is creating an entirely new generation of startups.\nThese generative AI startups have the ability to transform industries and shape the future, which is why today we announced a commitment of $230 million to accelerate the creation of generative AI applications by startups around the world. We are excited to collaborate with visionary startups, nurture their growth, and unlock new possibilities. In addition to this monetary investment, today we’re also announcing the second-annual AWS Generative AI Accelerator in partnership with NVIDIA. This global 10-week hybrid program is designed to propel the next wave of generative AI startups. This year, we’re expanding the program 4x to serve 80 startups globally. Selected participants will each receive up to $1 million in AWS Promotional Credits to fuel their development and scaling needs. The program also provides go-to-market support as well as business and technical mentorship. Participants will tap into a network that includes domain experts from AWS as well as key AWS partners such as NVIDIA, Meta, Mistral AI, and venture capital firms investing in generative AI.\nBuilding cloud services with Generative AI In addition to these programs, AWS is committed to making it possible for startups of all sizes and developers of all skill levels to build and scale generative AI applications with the most comprehensive set of capabilities across the three layers of the generative AI stack. At the bottom layer of the stack, we provide infrastructure to train large language models (LLMs) and foundation models (FMs) and produce inferences or predictions. This includes the best NVIDIA GPUs and GPU-optimized software, custom, machine learning (ML) chips including AWS Trainium and AWS Inferentia, as well as Amazon SageMaker, which greatly simplifies the ML development process. In the middle layer, Amazon Bedrock makes it easier for startups to build secure, customized, and responsible generative AI applications using LLMs and other FMs from leading AI companies. And at the top layer of the stack, we have Amazon Q, the most capable generative AI-powered assistant for accelerating software development and leveraging companies’ internal data.\nCustomers are innovating using technologies across the stack. For instance, during my time at the VivaTech conference in Paris last month, I sat down Michael Chen, VP of Strategic Alliances at PolyAI, which offers customized voice AI solutions for enterprises. PolyAI develops natural-sounding text-to-speech models using Amazon SageMaker. And they build on Amazon Bedrock to ensure responsible and ethical AI practices. They use Amazon Connect to integrate their voice AI into customer service operations.\nAt the bottom layer of the stack, NinjaTech uses Trainium and Inferentia2 chips, along with Amazon SageMaker, to build, train, and scale custom AI agents. From conducting research to scheduling meetings, these AI agents save time and money for NinjaTech’s users by bringing the power of generative AI into their everyday workflows. I recently sat down with Sam Naghshineh, Co-founder and CTO, to discuss how this approach enables them to save time and resources for their users.\nLeonardo.AI, a startup from the 2023 AWS Generative AI Accelerator cohort, is also harnessing the capabilities of AWS Inferentia2 to enable artists and professionals to produce high-quality visual assets with unmatched speed and consistency. By reducing their inference costs without sacrificing performance, Leonardo.AI can offer their most advanced generative AI features at a more accessible price point.\nLeading generative AI startups, including Perplexity, Hugging Face, AI21 Labs, Articul8, Luma AI, Hippocratic AI, Recursal AI, and DatologyAI are building, training, and deploying their models on Amazon SageMaker. For instance, Hugging Face used Amazon SageMaker HyperPod, a feature that accelerates training by up to 40%, to create new open-source FMs. The automated job recovery feature helps minimize disruptions during the FM training process, saving them hundreds of hours of training time a year.\nAt the middle layer, Perplexity leverages Amazon Bedrock with Anthropic Claude 3 to build their AI-powered search engine. Bedrock ensures robust data protection, ethical alignment through content filtering, and scalable deployment of Claude 3. While Nexxiot, an innovator in transportation and supply chain solutions, quickly moved its Scope AI assistant solution to Amazon Bedrock with Anthropic Claude in order to give their customers the best real-time, conversational insights into their transport assets.\nAt the top layer, Amazon Q Developer helps developers at startups build, test, and deploy applications faster and more efficiently, allowing them to focus their valuable energy on driving innovation. Ancileo, an insurance SaaS provider for insurers, re-insurers, brokers, and affinity partners, uses Amazon Q Developer to reduce the time to resolve coding-related issues by 30%, and is integrating ticketing and documentation with Amazon Q to speed up onboarding and allow anyone in the company to quickly find their answers. Amazon Q Businessenables everyone at a startup to be more data-driven and make better, faster decisions using the organization’s collective knowledge. Brightcove, a leading provider of cloud video services, deployed Amazon Q Business to streamline their customer support workflow, allowing the team to expedite responses, provide more personalized service, and ultimately enhance the customer experience.\nResources for generative AI startups The future of generative AI belongs to those who act now. The application window for the AWS Generative AI Accelerator program is open from June 13 to July 19, 2024, and we’ll be selecting a global cohort of the most promising generative AI startups. Don’t miss this unique chance to redefine what’s possible with generative AI, and apply now! Other helpful resources include:\nYou can use your AWS Activate credits for Amazon Bedrock to experiment with FMs, along with a broad set of capabilities needed to build responsible generative AI applications with security and privacy. Dive deeper by exploring our Generative AI Community space for technical content, insights, and connections with fellow builders. AWS also provides free training to help the current and future workforce take advantage of Amazon’s generative AI tools. For those interested in learning to build with generative AI on AWS, explore the comprehensive Generative AI Learning Plan for Developers to gain the skills you need to create cutting-edge applications NVIDIA offers NVIDIA Inception, a free program designed to help startups evolve faster through cutting-edge technology, opportunities to connect with venture capitalists, and access to the latest technical resources from NVIDIA. Apply now, explore the resources, and join the generative AI revolution with AWS. Additional Resources Twitch series: Let’s Ship It – with AWS! Generative AI AWS Generative AI Accelerator Program: apply now! TAGS: Accelerators , AWS Startups\nSource:\nhttps://aws.amazon.com/blogs/startups/accelerating-the-next-wave-of-generative-ai-startups/\n"},{"uri":"https://cbthien.github.io/fcj-workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Reflection Report: “AWS Cloud Mastery Series #2 – DevOps on AWS” Event Objectives Introduce DevOps services on AWS and how to design a CI/CD pipeline. Equip participants with a modern DevOps mindset and how to apply it in the AWS ecosystem. Explore concepts of Infrastructure as Code (IaC) and related tools (CloudFormation, CDK, Terraform). Provide an overview of container-based workloads on AWS (ECR, ECS, EKS, App Runner). Demonstrate how to achieve effective monitoring \u0026amp; observability using native AWS services. Optimize development speed, release quality, and system reliability. Speakers Truong Quang Tinh – AWS Community Builder, Platform Engineer (TymeX) Bao Huynh – AWS Community Builder Nguyen Khanh Phuc Thinh – AWS Community Builder Tran Dai Vi – AWS Community Builder Huynh Hoang Long – AWS Community Builder Pham Hoang Quy – AWS Community Builder Nghiem Le – AWS Community Builder Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey (Together with members from the FCJ Team, AWS Developer Advocate Team, and community guests.)\nKey Highlights Building a DevOps Mindset Foundation The speakers emphasized that DevOps is not just a job title but a mindset + set of working habits:\nAutomating repetitive tasks. Sharing knowledge across roles (Dev, Ops, QA, Security…). Continuously experimenting, learning, and improving (continuous learning). Making decisions based on measurable data, not just gut feeling. Common mistakes for beginners:\nOnly “following tutorials” without building real-world projects. Comparing themselves too much with others instead of focusing on small but consistent progress. Key messages repeated throughout the session:\nNo CI/CD → No DevOps.\nNo IaC → No DevOps.\nInfrastructure as Code (IaC) The tools/approaches were compared and analyzed:\nCloudFormation\nNative AWS service, declarative model. Suitable for many enterprise teams wanting to stay fully AWS-native. AWS CDK\nDefine infrastructure using programming languages (TypeScript, Python, Java, …). Model: Constructs → Stacks → Apps. Easy to modularize, reuse, and test; very developer-friendly and fits Agile teams. Terraform\nSuitable for multi-cloud / hybrid scenarios. Manages state files and supports many providers. Concepts were explained through practical examples:\nStack, Construct, State file, YAML template structure (Resources, Parameters, Outputs, Mappings)… Important message:\nInfrastructure defined as IaC becomes consistent, version-controlled, easy to review, and easy to roll back, much better than manual click-ops in the console. AWS DevOps Services – CI/CD Pipeline The content focused on building an AWS-standard pipeline:\nCI/CD on AWS:\nCodePipeline – orchestrates the entire pipeline. CodeBuild – builds, tests, linting, static analysis (buildspec.yml, build caching, parallel tests). CodeDeploy – deploys applications (blue/green, canary, rolling). End-to-end pipeline demo:\nCommit → Build → Test → Deploy → Monitor. Emphasis: deployment must be 100% automated, no manual “click deploy” in the console. DevOps strategy:\nStart with a small pipeline, then expand gradually. Use blue/green and canary deployments to reduce risk. Combine IaC + GitOps for multi-team environments. Containers and Deployment Models on AWS Docker fundamentals Dockerfile → Build → Image → Registry → Run container Registries: Docker Hub or Amazon ECR Amazon ECR Stores container images. Supports image scanning, lifecycle policies, and per-repository permissions. Amazon ECS Orchestrates containers with Fargate or EC2. Integrates with Application Load Balancer, autoscaling based on CPU, memory, and queue length. Amazon EKS Managed Kubernetes service on AWS. Suitable for large-scale, multi-team environments requiring portability and the full Kubernetes ecosystem. AWS App Runner A service to “deploy containers like deploying to Vercel/Netlify” for backend/web applications. Suitable for teams that want to minimize infrastructure/cluster management. Case study \u0026amp; comparison of ECS, EKS, App Runner:\nECS: easy to use, AWS-native, suitable for most container workloads on AWS. EKS: ideal when already using Kubernetes or requiring multi-cloud portability. App Runner: great for small teams, startups, and feature teams who want to focus on code. Monitoring \u0026amp; Observability This part focused on CloudWatch and X-Ray:\nAmazon CloudWatch Collects Metrics, Logs, Dashboards. Supports composite alarms, log filters, and custom metrics for business KPIs. AWS X-Ray Distributed tracing: draws service maps and traces end-to-end requests. Very useful for debugging latency and bottlenecks in microservices. Best practices:\nDesign alerts carefully to avoid “alert noise”. Build clear on-call workflows and runbooks. Monitor using the Golden Signals: Latency, Traffic, Errors, Saturation. Highlighted message:\nNo observability = no idea how your system is dying.\nKey Learnings DevOps is not just a job title; it’s a mindset and a set of habits: automation, sharing, and measurable outcomes. IaC helps infrastructure become: Consistent Repeatable Easy to maintain, audit, and roll back Choosing IaC tools (CloudFormation / CDK / Terraform) should be based on: Team requirements Project needs Complexity and environment (AWS-only vs multi-cloud). Understanding the differences between container services (ECR, ECS, EKS, App Runner) helps: Pick the right service for the right workload. Monitoring \u0026amp; Observability are not “add-ons at the end” but must be designed from day one. Gained more familiarity with concepts such as: DORA metrics, CI → CD → Continuous Learning Blue/green and canary deployments GitOps, incident workflows, MTTR… Applying to Work Example: AI Chatbot Project on AWS If I have the opportunity to build an AI Chatbot on AWS, I plan to apply:\nCI/CD pipeline design:\nUse CodePipeline + CodeBuild + CodeDeploy to automate build, test, and deployment. Every commit to the chatbot code (backend, Lambda, API) will be tested and deployed automatically. Infrastructure as Code:\nUse AWS CDK to define the entire infrastructure: Lambda, API Gateway, DynamoDB, S3, IAM, EventBridge, … Everything is version-controlled in Git, making it easy to reuse and scale. Containerization \u0026amp; deployment:\nPackage some services (e.g., RAG API, vector DB interface) into Docker images. Deploy to ECS (Fargate) or App Runner, depending on scaling needs. Monitoring \u0026amp; Incident handling:\nUse CloudWatch metrics + logs + dashboards to monitor the chatbot. Apply X-Ray if the architecture is microservices-based. Set up an incident workflow: alert → investigate → fix → postmortem (runbook, on-call). By applying DevOps practices and AWS services, an AI chatbot system can:\nDevelop faster (higher dev velocity). Deploy frequently while staying safe. Be easier to maintain and scale as user traffic grows. Event Experience The event gave me a more realistic view of how modern organizations implement DevOps on AWS. The speakers not only covered theory but also shared plenty of real-world examples, from IaC and CI/CD to container orchestration. I got to see demos of: A pipeline from commit → live deployment. Drift detection, cdk synth/deploy. Real-time container deployments on ECS/ECR. It was also a great opportunity to connect with like-minded peers, exchange DevOps, AWS, and Cloud learning experiences. Takeaways No CI/CD → No DevOps. IaC is a prerequisite for effective automation. Containers = scalability + portability + speed. Observability = reliability – if you can’t observe it, you can’t trust it to stay healthy in production. In summary, “AWS Cloud Mastery Series #2 – DevOps on AWS” not only helped me solidify DevOps concepts in theory, but also clarified how to build automated, scalable, and observable systems on AWS, forming a strong foundation for future Cloud \u0026amp; DevOps projects.\nSome photos from the event "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.2-preparation/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Requirements needed to complete this workshop:\nAWS Client Machine: Configured with access to necessary AWS services Development Environment: Windows, macOS, or Linux with basic development tools Basic Knowledge: Understanding of AWS, Python, JavaScript, and Docker GitHub Account: To clone source code and track changes AWS Budget: Approximately $65/month for resources (EC2, Bedrock, NAT Gateway) Tool Installation 1. AWS CLI AWS Command Line Interface (AWS CLI) is a tool to interact with AWS services.\nWindows:\n# Download and install MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\nbrew install awscli Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Verify installation:\naws --version # aws-cli/2.x.x Python/3.x.x 2. Terraform Terraform is an Infrastructure as Code tool to provision AWS resources.\nWindows:\nchoco install terraform macOS:\nbrew tap hashicorp/tap brew install hashicorp/tap/terraform Linux:\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update \u0026amp;\u0026amp; sudo apt install terraform Verify:\nterraform --version 3. Docker Docker to run Qdrant vector database locally and on EC2.\nWindows/macOS: Download Docker Desktop\nLinux:\ncurl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -aG docker $USER Verify:\ndocker --version docker run hello-world 4. Node.js (\u0026gt;= 18) Node.js for frontend development with React + Vite.\nUsing nvm (recommended):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js 18 nvm install 18 nvm use 18 Verify:\nnode --version # v18.x.x npm --version # 9.x.x 5. Python (\u0026gt;= 3.11) Python for backend FastAPI application.\nWindows: Download from python.org (select \u0026ldquo;Add to PATH\u0026rdquo;)\nmacOS:\nbrew install python@3.11 Linux:\nsudo apt update sudo apt install python3.11 python3.11-venv python3-pip Verify:\npython --version # Python 3.11.x pip --version 6. Git Git for version control.\nWindows: Download from git-scm.com\nmacOS:\nbrew install git Linux:\nsudo apt install git Verify:\ngit --version Clone Repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Configure AWS Credentials Create IAM User Login to AWS Console Navigate to IAM → Users → Create user User name: arc-workshop-user Attach policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Create access key → Download credentials Configure AWS CLI aws configure Enter information:\nAWS Access Key ID: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name: ap-southeast-1 Default output format: json Verify:\naws sts get-caller-identity Activate Amazon Bedrock Models Request Model Access AWS Console → Amazon Bedrock → Model access Click Manage model access Select models: Anthropic - Claude 3.5 Sonnet (anthropic.claude-3-5-sonnet-20241022-v2:0) Cohere - Embed Multilingual v3 (cohere.embed-multilingual-v3) Click Request model access → Accept Terms → Submit Verify Access # Test Claude aws bedrock get-foundation-model \\ --model-identifier anthropic.claude-3-5-sonnet-20241022-v2:0 \\ --region ap-southeast-1 # Test Cohere aws bedrock get-foundation-model \\ --model-identifier cohere.embed-multilingual-v3 \\ --region ap-southeast-1 Expected: Status Access granted\nPrepare Sample Documents Project comes with sample PDFs in samples/:\nls samples/ # data-structures-sample.pdf # test-sample.pdf Document Requirements Limit Value Format PDF (text-based or scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Checklist Before proceeding, ensure:\nAWS CLI installed and configured Terraform installed Docker installed and running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created with proper permissions Bedrock models approved (Claude + Cohere) Sample documents ready "},{"uri":"https://cbthien.github.io/fcj-workshop/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Academic Research Chatbot AWS RAG-based solution for smart academic support and research 1. Executive Summary Academic Research Chatbot is an AI assistant supporting academic research, helping students and lecturers search, summarize, and analyze scientific documents (PDFs, papers) through natural conversation with accurate source citations.\nKey Highlights:\nCore Technology: Combines IDP (Amazon Textract) to process documents (including scans) and RAG (Amazon Bedrock - Claude 3.5 Sonnet) to generate intelligent responses. Optimized Architecture: Hybrid model using 1 EC2 t3.small combined with Serverless services (Amplify, Cognito, S3, DynamoDB) to balance performance and cost. Feasibility: Serves ~50 internal users with operating costs ~$60/month, fast deployment time (20 days), and maximizes AWS Free Tier. 2. Problem Statement Current Problem Students and researchers have to work with a large number of academic documents (conference papers, journals, theses, technical reports). Many documents are old scanned PDFs (pre-2000), without a text layer, making searching for content, data, and tables very time-consuming. Public AI tools (ChatGPT, Perplexity, NotebookLM, etc.) are not directly connected to the school/department\u0026rsquo;s internal document repository, making it difficult to ensure security and access rights by subject or research group. The current infrastructure lacks a unified access point to:\nManage research documents by subject/topic. Allow researchers to ask questions directly on their own papers. Ensure answers have clear citations (paper, page, table, section). Consequence: Researchers have to manually read, take notes, and copy data from multiple papers; lecturers find it difficult to quickly synthesize information when preparing lectures or topics; academic data is scattered across many personal machines, difficult to standardize and reuse. Solution Academic Research Chatbot proposes building an internal academic Q\u0026amp;A platform based on AWS, where:\nDev/Admin loads research document repository: Upload PDFs to Amazon S3, metadata is stored in Amazon DynamoDB. An EC2 worker consumes the Amazon SQS queue, calls Amazon Textract to OCR, extract text, tables, forms, including scanned documents. Worker normalizes/chunks content, sends to Amazon Bedrock Titan Text Embeddings v2 to generate embeddings, and indexes into Qdrant on EC2. Researchers ask questions via web interface (Amplify + CloudFront): Questions are embedded, querying Qdrant to retrieve the most relevant segments (Retrieval). These segments are passed to Claude 3.5 Sonnet on Amazon Bedrock to generate answers with accurate citations (paper, page, section, table) and explanations in academic context. All access is protected by Amazon Cognito (researcher vs admin authorization), logs \u0026amp; metrics are monitored via Amazon CloudWatch + SNS (alerts on worker errors, queue backlog, high EC2 CPU). Benefits and ROI Academic Efficiency:\nReduces 40–60% of time researchers spend finding data, F1-scores, p-values, sample sizes, experimental equipment, or method descriptions from multiple papers. Reduces citation errors due to forgetting pages/tables, as the chatbot always returns sources and locations. Internal Knowledge Management: Research documents are centralized in an S3 + DynamoDB repository, easy to backup, authorize, and expand. Can be reused for many courses, topics, and labs without building a new system. Low \u0026amp; Controllable Infrastructure Costs: Hybrid model 1 EC2 + managed AI services keeps operating costs for 50 internal users at around \u0026lt; $50/month, mainly paying for EC2, 2–3 VPC endpoint interfaces, and Bedrock/Textract usage. The system is designed to be deployed in about 20 days by a team of 4, suitable for a research/internship project but still has product architecture quality. Long-term Value: Creates a platform to later integrate learning behavior analysis dashboards, paper recommendation modules, or expand to multi-language and multi-field learning assistants. 3. Solution Architecture Academic Research Chatbot applies the AWS Hybrid RAG Architecture model with IDP (Intelligent Document Processing), combining a single EC2 (FastAPI + Qdrant + Worker) with managed AI services (Textract, Bedrock) to optimize costs while ensuring performance for about 50 internal users.\nData Processing and Conversation Flow AWS Services Used\nAmazon Route 53: DNS management for chatbot platform domain. Amazon CloudFront: CDN distributing web interface (chat + admin) with low latency. AWS Amplify Hosting: Hosts web application (React/Next) for Researchers and Dev/Admin. Amazon Cognito: User authentication, researcher vs admin role management. Amazon S3: Stores original PDF files uploaded by Dev/Admin (raw documents). Amazon SQS (doc_ingestion_queue): Job queue for document processing. Amazon Textract: IDP/OCR for scanned PDFs and digital PDFs. Amazon Bedrock: Titan Text Embeddings v2: Generates embedding vectors for text chunks. Claude 3.5 Sonnet: Generates academic answers from context + user questions (RAG). Amazon DynamoDB: Documents table: document metadata, pipeline status (UPLOADED, IDP_RUNNING, EMBEDDING_DONE, FAILED). Amazon EC2 (t3.small, private subnet): Runs FastAPI backend (REST API for chat and admin). Runs Qdrant Vector DB to store and query embeddings. Runs Worker process consuming SQS, calling Textract + Titan, indexing into Qdrant, updating DynamoDB. VPC + ALB + VPC Endpoints: VPC + private subnet for EC2 (not directly exposed to Internet). Application Load Balancer (ALB): entry point for all APIs from Amplify to EC2. Gateway Endpoint (S3, DynamoDB) and Interface Endpoint (Textract, Bedrock, SQS – if used) for EC2 to call AWS services without NAT Gateway. Amazon CloudWatch + Amazon SNS: Collects logs and metrics from EC2, ALB, SQS. CloudWatch Alarms send alerts via SNS when CPU is high, SQS backlog, worker errors, etc. AWS CodePipeline / CodeBuild: Automates build \u0026amp; deploy for backend (FastAPI on EC2). Component Design\nUsers: Researchers: Q\u0026amp;A, academic content lookup. Dev/Admin: Upload, manage, and re-index documents. Document Processing (IDP): PDFs uploaded by Dev/Admin to S3. Worker on EC2 calls Textract for OCR and text/table extraction. Indexing \u0026amp; Vector DB: Worker normalizes, chunks content. Calls Bedrock Titan Embeddings v2 to create embeddings. Saves embeddings + metadata to Qdrant on EC2. AI Conversation (RAG): FastAPI embeds question, queries Qdrant for top-k relevant segments. Sends context + question to Claude 3.5 Sonnet (Bedrock) to generate answer with citation. User Management: Cognito authenticates and authorizes researcher / admin. Storage \u0026amp; State: DynamoDB stores document metadata (doc_id, status, owner, …) and (optional) chat history. 4. Technical Implementation Implementation Phases The project consists of 2 main parts — web platform (UI + auth) and RAG + IDP backend — deployed across 4 phases:\nResearch \u0026amp; Architecture Finalization: Review requirements (50 researchers, 1 EC2, IDP + RAG). Finalize architecture: VPC, EC2 (FastAPI + Qdrant + Worker), Amplify, Cognito, S3, SQS, DynamoDB, Textract, Bedrock. POC \u0026amp; Connectivity Check: Create EC2, VPC endpoints, test calling Textract, Titan Embeddings, Claude 3.5 Sonnet. Run simple Qdrant on EC2, test vector insert/search. Create skeleton FastAPI + a minimal Chat UI on Amplify. Feature Completion: Build /api/chat (FastAPI) + RAG pipeline: embed query → Qdrant → Claude + citation. Build /api/admin/: upload PDF, save to S3 + DynamoDB, push message to SQS. Write Worker on EC2: SQS → Textract → normalize/chunk → Titan → Qdrant → update DynamoDB. Complete Chat UI and Admin UI (upload + view document status). Testing, Optimization, Internal Demo Deployment: End-to-end test with a set of ~50–100 papers. Add CloudWatch Logs/Alarms, SNS notify on error or queue backlog. Adjust EC2, Qdrant configuration, batch size to optimize time and cost. Prepare user guide and demo for the group of 50 researchers. Technical Requirements\nFrontend \u0026amp; Auth: React/Next.js hosted on AWS Amplify, CloudFront CDN, Route 53 DNS. Amazon Cognito manages identity and permissions (Researcher/Admin). Backend \u0026amp; Compute: EC2 t3.small (Private Subnet) running All-in-one: FastAPI, Qdrant Vector DB, and Worker. Asynchronous processing: Worker reads SQS, triggers Textract and Bedrock to index data. IDP \u0026amp; RAG: Storage: S3 (Original files), DynamoDB (Metadata \u0026amp; Status). AI Core: Textract (OCR scanned docs), Bedrock Titan (Embedding), Claude 3.5 Sonnet (Question Answering). Network \u0026amp; Observability: Network: VPC Private Subnet, VPC Endpoints for secure connection to AWS Services. Monitoring: CloudWatch Logs/Metrics + SNS alerts (High CPU, Worker errors). 5. Timeline \u0026amp; Milestones The project is executed over approximately 6 weeks with specific phases:\nWeek 1-2 (Days 1-10): Research \u0026amp; Design Detailed architecture design, scope definition, service selection. Planning for operational cost optimization and deployment. Week 3 (Days 11-15): AWS Infrastructure Setup Configure VPC, Subnets, Security Groups, IAM Roles. Deploy EC2 t3.small, S3 bucket, DynamoDB tables. Setup VPC Endpoints (Gateway + Interface). Week 4 (Days 16-20): Backend APIs \u0026amp; IDP Pipeline Build FastAPI endpoints (/api/chat, /api/admin/upload). Integrate IDP pipeline: SQS → Worker → Textract → Embeddings → Qdrant. Connect Bedrock (Titan Embeddings + Claude 3.5 Sonnet). Week 5 (Days 21-25): Testing \u0026amp; Error Handling End-to-end testing with a set of ~50-100 papers. Handle edge cases, retry logic, error handling. Optimize chunking strategy and retrieval accuracy. Week 6 (Days 26-30): Deployment \u0026amp; Documentation Finalize UI/UX for Admin and Researcher. Setup CloudWatch Alarms + SNS notifications. Prepare user guide and demo for the group of 50 researchers. 6. Budget Estimation You can view costs on the AWS Pricing Calculator Or download the Budget Estimation File.\nInfrastructure Costs (Estimated Monthly)\nFixed Infrastructure (~$40–45): Compute \u0026amp; Network: EC2 t3.small ($15) + VPC Endpoints ($20). Storage \u0026amp; Web: S3, DynamoDB, SQS, Amplify, CloudWatch (~$5–10). AI Costs (Variable): Amazon Textract: ~$15–25 (batch processing first 10,000 pages). Amazon Bedrock: $5–15 (serving 50 users). Total: **$50–60/month** for internal research environment. 7. Risk Assessment Risk Matrix\nHallucination (AI fabrication): High impact, medium probability. Budget Overrun (AI Services): Medium impact, medium probability. Infrastructure Failure (EC2/Qdrant): High impact, low probability. Mitigation Strategies\nAI Quality: Mandatory source citations, limit input context from Qdrant. Cost: Set up AWS Budgets/Alarms, control document ingestion volume. Infrastructure \u0026amp; Security: Periodic EBS backups, data encryption (S3/DynamoDB), strict permissions via Cognito/IAM. Contingency Plans\nSystem Failure: Restore from Snapshot, pause ingestion (buffer via SQS). Cost Overrun: Temporarily lock new uploads, limit daily query quotas. 8. Expected Outcomes Technical Improvements\nTransform scattered document repositories (PDF/Scan) into digital knowledge queryable and automatically citable. Significantly reduce manual search time thanks to RAG + IDP technology. Long-term Value\nBuild a digitized research platform for 50+ researchers, easily scalable. Create a foundation for advanced features: Document recommendations, research trend analysis, and Literature Review support. "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Deepen understanding of AWS IAM access control: Users, Groups, Policies, and Roles. Learn how to design and implement secure access patterns using IAM Roles and least privilege. Practice creating IAM Groups, Users, Roles and configuring role switching scenarios. Understand core Amazon VPC networking concepts: Subnets, Route Tables, Internet Gateway, NAT Gateway. Get hands-on experience with VPC security (Security Groups, Network ACLs) and an introduction to Site-to-Site VPN. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study IAM access control overview + IAM Users \u0026amp; IAM Groups + IAM Policies + IAM Roles - Review security objectives and principle of least privilege 23/09/2025 23/09/2025 https://000002.awsstudygroup.com/ 2 - Hands-on IAM (1): + Create Admin IAM Group + Create Admin User and add to group + Login as Admin User and verify permissions 24/09/2025 24/09/2025 https://000002.awsstudygroup.com/ 3 - Hands-on IAM (2): + Create Admin Role + Create OperatorUser + Configure trust relationships for role switching + Test “Switch Role” from OperatorUser + Review and clean up config 25/09/2025 25/09/2025 https://000002.awsstudygroup.com/ 4 - Study Amazon VPC fundamentals: + Subnets + Route Tables + Internet Gateway + NAT Gateway - Learn VPC security components: + Security Groups + Network ACLs 26/09/2025 26/09/2025 https://000003.awsstudygroup.com/ 5 - Hands-on VPC \u0026amp; Networking: + Create VPC, Subnets, Internet Gateway, Route Tables, Security Groups + Enable VPC Flow Logs + Launch EC2 instance in VPC and test connectivity + Read through Site-to-Site VPN setup steps 27/09/2025 27/09/2025 https://000003.awsstudygroup.com/ Week 3 Achievements: Gained a solid understanding of IAM access control concepts:\nIAM Users and IAM Groups IAM Policies (permission-based access) IAM Roles and trust relationships Implemented a basic IAM administrative structure:\nCreated Admin Group and Admin User Verified permissions using group-based policies Practiced advanced IAM patterns:\nCreated Admin Role and OperatorUser Configured and tested “Switch Role” flows Applied the principle of least privilege when assigning permissions Understood Amazon VPC core components:\nSubnets, Route Tables, Internet Gateway, NAT Gateway Security Groups vs. Network ACLs Built a small VPC environment:\nCreated VPC, subnets, routing, and security layers Launched EC2 instance inside VPC and verified connectivity Enabled VPC Flow Logs for traffic visibility Reviewed the overall steps to set up AWS Site-to-Site VPN for hybrid connectivity.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/3-blogstranslated/3.3-blog3/","title":"How climate tech startups build foundation models with Amazon SageMaker HyperPod","tags":[],"description":"","content":"by Ilan Gleiser, Aman Shanbhag, Ankit Anand, Lisbeth Kaufman, and Rohit Talluri | on 04 JUN 2025 | in Best Practices , Responsible AI, Startup, Sustainability\nClimate tech startups are companies that use technology and innovation to address the climate crisis, with a primary focus on either reducing greenhouse gas emissions or helping society adapt to climate change impacts. Their unifying mission is to create scalable solutions that accelerate the transition to a sustainable, low-carbon future. Solutions to the climate crisis are ever more important as climate-driven extreme weather disasters increase globally. In 2024, climate disasters caused more than $417B in damages globally, and there’s no slowing down in 2025 with LA wildfires that destroyed more than $135B in the first month of the year alone. Climate tech startups are at the forefront of building impactful solutions to the climate crisis, and they’re using generative AI to build as quickly as possible.\nIn this post, we show how climate tech startups are developing foundation models (FMs) that use extensive environmental datasets to tackle issues such as carbon capture, carbon-negative fuels, new materials design for microplastics destruction, and ecosystem preservation. These specialized models require advanced computational capabilities to process and analyze vast amounts of data effectively.\nAmazon Web Services (AWS) provides the essential compute infrastructure to support these endeavors, offering scalable and powerful resources through Amazon SageMaker HyperPod. SageMaker HyperPod is a purpose-built infrastructure service that automates the management of large-scale AI training clusters so developers can efficiently build and train complex models such as large language models (LLMs) by automatically handling cluster provisioning, monitoring, and fault tolerance across thousands of GPUs. With SageMaker HyperPod, startups can train complex AI models on diverse environmental datasets, including satellite imagery and atmospheric measurements, with enhanced speed and efficiency. This computational backbone is vital for startups striving to create solutions that are not only innovative but also scalable and impactful.\nThe increasing complexity of environmental data demands robust data infrastructure and sophisticated model architectures. Integrating multimodal data, employing specialized attention mechanisms for spatial-temporal data, and using reinforcement learning are crucial for building effective climate-focused models. SageMaker HyperPod optimized GPU clustering and scalable resources help startups save time and money while meeting advanced technical requirements, which means they can focus on innovation. As climate technology demands grow, these capabilities allow startups to develop transformative environmental solutions using Amazon SageMaker HyperPod.\nTrends among climate tech startups building with generative AI climate tech startups adopting generative AI to optimize operations. For example, startups such as BrainBox AI and Pendulum used Amazon Bedrock and fine-tuned existing LLMs on AWS Trainium using Amazon SageMaker to more rapidly onboard new customers through automated document ingestion and data extraction. Midway through 2023, we saw the next wave of climate tech startups building sophisticated intelligent assistants by fine-tuning existing LLMs for specific use cases. For example, NET2GRID used Amazon SageMaker for fine-tuning and deploying scale-based LLMs based on Llama 7B to build EnergyAI, an assistant that provides quick, personalized responses to utility customers’ energy-related questions.\nOver the last 6 months, we’ve seen a flurry of climate tech startups building FMs that address specific climate and environmental challenges. Unlike language-based models, these startups are building models based on real-world data, like weather or geospatial earth data. Whereas LLMs such as Anthropic’s Claude or Amazon Nova have hundreds of billions of parameters, climate tech startups are building smaller models with just a few billion parameters. This means these models are faster and less expensive to train. We’re seeing some emerging trends in use cases or climate challenges that startups are addressing by building FMs. Here are the top use cases, in order of popularity:\nWeather – Trained on historic weather data, these models offer short-term and long-term, hyperaccurate, hyperlocal weather and climate predictions, some focusing on specific weather elements like wind, heat, or sun.\nSustainable materials discovery – Trained on scientific data, these models invent new sustainable material that solve specific problems, like more efficient direct air capture sorbents to reduce the cost of carbon removal or molecules to destroy microplastics from the environment.\nNatural ecosystems – Trained on a mix of data from satellites, lidar, and on-the ground sensors, these models offer insights into natural ecosystems, biodiversity, and wildfire predictions.\nGeological modeling – Trained on geological data, these models help determine the best locations for geothermal or mining operations to reduce waste and save money.\nTo better understand these trends, the following sections take a closer look at how climate startups are building foundation models on AWS.\nOrbital Materials: Foundation models for sustainable materials discovery Orbital Materials has built a proprietary AI platform to design, synthesize, and test new sustainable materials. Developing new advanced materials has traditionally been a slow process of trial and error in the lab. Orbital replaces this with generative AI design, radically speeding up materials discovery and new technology commercialization. They’ve released a generative AI model called “Orb” that suggests new material design, which the team then tests and perfects in the lab.\nOrb is a diffusion model that Orbital Materials trained from scratch using SageMaker HyperPod. The first product the startup designed with Orb is a sorbent for carbon capture in direct air capture facilities. Since establishing its lab in the first quarter of 2024, Orbital has achieved a tenfold improvement in its material’s performance using its AI platform—an order of magnitude faster than traditional development and breaking new ground in carbon removal efficacy. By improving the performance of the materials, the company can help drive down the costs of carbon removal, which can enable rapid scale-up. They chose to use SageMaker HyperPod because they “like the one-stop shop for control and monitoring,” explained Jonathan Godwin, CEO of Orbital Material. Orbital was able to reduce their total cost of ownership (TCO) for their GPU cluster with Amazon SageMaker HyperPod deep health checks for stress testing their GPU instances to swap out faulty nodes. Moreover, Orbital can use SageMaker HyperPod to automatically swap out failing nodes and restart model training from the last saved checkpoint, freeing up time for the Orbital Materials team. The SageMaker HyperPod monitoring agent continually monitors and detects potential issues, including memory exhaustion, disk failures, GPU anomalies, kernel deadlocks, container runtime issues, and out-of-memory (OOM) crashes. Based on the underlying issue the monitoring agent either replaces or reboots the node.\nWith the launch of SageMaker HyperPod on Amazon Elastic Kubernetes Service (Amazon EKS), Orbital can set up a unified control plane consisting of both CPU-based workloads and GPU-accelerated tasks within the same Kubernetes cluster. This architectural approach eliminates the traditional complexity of managing separate clusters for different compute resources, significantly reducing operational overhead. Orbital can also monitor the health status of SageMaker HyperPod nodes through Amazon CloudWatch Container Insights with enhanced observability for Amazon EKS. Amazon CloudWatch Container Insights collects, aggregates, and summarizes metrics and logs from containerized applications and microservices, providing detailed insights into performance, health, and status metrics for CPU, GPU, Trainium, or Elastic Fabric Adapter (EFA) and file system up to the container level.\nAWS and Orbital Materials have established a deep partnership that enables fly-wheel growth. The companies have entered a multiyear partnership, in which Orbital Material builds its FMs with SageMaker HyperPod and other AWS services. In return, Orbital Materials is using AI to develop new data center decarbonization and efficiency technologies. To further spin the fly-wheel, Orbital will be making its market-leading open source AI model for simulating advanced materials, Orb, generally available for AWS customers by using Amazon SageMaker JumpStart and AWS Marketplace. This marks the first AI-for-materials model to be on AWS platforms. With Orb, AWS customers working on advanced materials and technologies such as semiconductors, batteries, and electronics can access market-leading accelerated research and development (R\u0026amp;D) within a secure and unified cloud environment.\nThe architectural advantages of SageMaker HyperPod on Amazon EKS are demonstrated in the following diagram. The diagram illustrates how Orbital can establish a unified control plane that manages both CPU-based workloads and GPU-accelerated tasks within a single Kubernetes cluster. This streamlined architecture eliminates the traditional complexity of managing separate clusters for different compute resources, providing a more efficient and integrated approach to resource management. The visualization shows how this consolidated infrastructure enables Orbital to seamlessly orchestrate their diverse computational needs through a single control interface.\nHum.AI: Foundation models for earth observation Hum.AI is building generative AI FMs that provide general intelligence of the natural world. Customers can use the platform to track and predict ecosystems and biodiversity to understand business impact and better protect the environment. For example, they work with coastal communities who use the platform and insights to restore coastal ecosystems and improve biodiversity.\nHum.AI’s foundation model looks at natural world data and learns to represent it visually. They’re training on 50 years of historic data collected by satellites, which amounts to thousands of petabytes of data. To accommodate processing this massive dataset, they chose SageMaker HyperPod for its scalable infrastructure. Through their innovative model architecture, the company achieved the ability to see underwater from space for the very first time, overcoming the historical challenges posed by water reflections\nHum.AI’s FM architecture employs a variational autoencoder (VAE) and generative adversarial network (GAN) hybrid design, specifically optimized for satellite imagery analysis. It’s an encoder-decoder model, where the encoder transforms satellite data into a learned latent space, while the decoder reconstructs the imagery (after being processed in the latent space), maintaining consistency across different satellite sources. The discriminator network provides both adversarial training signals and learned feature-wise reconstruction metrics. This approach helps preserve important ecosystem details that would otherwise be lost with traditional pixel-based comparisons, particularly for underwater environments, where water reflections typically interfere with visibility.\nUsing SageMaker HyperPod to train such a complex model enables Hum.AI to efficiently process their personally curated SeeFar dataset through distributed training across multiple GPU-based instances. The model simultaneously optimizes both VAE and GAN objectives across GPUs. This, paired with the SageMaker HyperPod auto-resume feature that automatically resumes a training run from the latest checkpoint, provides training continuity, even through node failures.\nHum.AI also used the SageMaker HyperPod out-of-the-box comprehensive observability features through Amazon Managed Service for Prometheus and Amazon Managed Service for Grafana for metric tracking. For their distributed training needs, they used dashboards to monitor cluster performance, GPU metrics, network traffic, and storage operations. This extensive monitoring infrastructure enabled Hum.AI to optimize their training process and maintain high resource utilization throughout their model development.\n\u0026ldquo;Our decision to use SageMaker HyperPod was simple; it was the only service out there where you can continue training through failure. We were able to train larger models faster by taking advantage of the large-scale clusters and redundancy offered by SageMaker HyperPod. We were able to execute experiments faster and iterate models at speeds that were impossible prior to SageMaker HyperPod. SageMaker HyperPod took all of the worry out of large-scale training failures. They’ve built the infrastructure to hot swap GPUs if anything goes wrong, and it saves thousands in lost progress between checkpoints. The SageMaker HyperPod team personally helped us set up and execute large training rapidly and easily.\u0026rdquo;\n— Kelly Zheng, CEO of Hum.AI\nHum.AI’s innovative approach to model training is illustrated in the following figure. The diagram showcases how their model simultaneously optimizes both VAE and GAN objectives across multiple GPUs. This distributed training strategy is complemented by the SageMaker HyperPod auto-resume feature, which automatically restarts training runs from the latest checkpoint. Together, these capabilities provide continual and efficient training, even in the face of potential node failures. The image provides a visual representation of this robust training process, highlighting the seamless integration between Hum.AI’s model architecture and SageMaker HyperPod infrastructure support.\nHow to save time and money building with Amazon SageMaker HyperPod Amazon SageMaker HyperPod removes the undifferentiated heavy lifting for climate tech startups building FMs, saving them time and money. For more information on how SageMaker HyperPod’s resiliency helps save costs while training, check out Reduce ML training costs with Amazon SageMaker HyperPod.\nAt its core is deep infrastructure control optimized for processing complex environmental data, featuring secure access to Amazon Elastic Compute Cloud (Amazon EC2) instances and seamless integration with orchestration tools such as Slurm and Amazon EKS. This infrastructure excels at handling multimodal environmental inputs, from satellite imagery to sensor network data, through distributed training across thousands of accelerators.\nThe intelligent resource management available in SageMaker HyperPod is particularly valuable for climate modeling, automatically governing task priorities and resource allocation while reducing operational overhead by up to 40%. This efficiency is crucial for climate tech startups processing vast environmental datasets because the system maintains progress through checkpointing while making sure that critical climate modeling workloads receive necessary resources.\nFor climate tech innovators, the SageMaker HyperPod library of over 30 curated model training recipes accelerates development, allowing teams to begin training environmental models in minutes rather than weeks. The platform’s integration with Amazon EKS provides robust fault tolerance and high availability, essential for maintaining continual environmental monitoring and analysis.\nSageMaker HyperPod flexible training plans are particularly beneficial for climate tech projects, allowing organizations to specify completion dates and resource requirements while automatically optimizing capacity for complex environmental data processing. The system’s ability to suggest alternative plans provides optimal resource utilization for computationally intensive climate modeling tasks.With support for next-generation AI accelerators such as the AWS Trainium chips and comprehensive monitoring tools, SageMaker HyperPod provides climate tech startups with a sustainable and efficient foundation for developing sophisticated environmental solutions. This infrastructure enables organizations to focus on their core mission of addressing climate challenges while maintaining operational efficiency and environmental responsibility.\nPractices for sustainable computing Climate tech companies are especially aware of the importance of sustainable computing practices. One key approach is the meticulous monitoring and optimization of energy consumption during computational processes. By adopting efficient training strategies, such as reducing the number of unnecessary training iterations and employing energy-efficient algorithms, startups can significantly lower their carbon footprint.\nAdditionally, the integration of renewable energy sources to power data centers plays a crucial role in minimizing environmental impact. AWS is determined to make the cloud the cleanest and the most energy-efficient way to run all our customers’ infrastructure and business. We have made significant progress over the years. For example, Amazon is the largest corporate purchaser of renewable energy in the world, every year since 2020. We’ve achieved our renewable energy goal to match all the electricity consumed across our operations—including our data centers—with 100% renewable energy, and we did this 7 years ahead of our original 2030 timeline.\nCompanies are also turning to carbon-aware computing principles, which involve scheduling computational tasks to coincide with periods of low carbon intensity on the grid. This practice means that the energy used for computing has a lower environmental impact. Implementing these strategies not only aligns with broader sustainability goals but also promotes cost efficiency and resource conservation. As the demand for advanced computational capabilities grows, climate tech startups are becoming vigilant in their commitment to sustainable practices so that their innovations contribute positively to both technological progress and environmental stewardship.\nConclusion Amazon SageMaker HyperPod is emerging as a crucial tool for climate tech startups in their quest to develop innovative solutions to pressing environmental challenges. By providing scalable, efficient, and cost-effective infrastructure for training complex multimodal and multi- model architectures, SageMaker HyperPod enables these companies to process vast amounts of environmental data and create sophisticated predictive models. From Orbital Materials’ sustainable material discovery to Hum.AI’s advanced earth observation capabilities, SageMaker HyperPod is powering breakthroughs that were previously out of reach. As climate change continues to pose urgent global challenges, SageMaker HyperPod automated management of large-scale AI training clusters, coupled with its fault-tolerance and cost-optimization features, allows climate tech innovators to focus on their core mission rather than infrastructure management. By using SageMaker HyperPod, climate tech startups are not only building more efficient models—they’re accelerating the development of powerful new tools in our collective effort to address the global climate crisis.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - FotMob delivers near real-time football updates to millions of fans with AWS The article describes how FotMob – a soccer app with over 17 million users across 500 leagues – uses AWS infrastructure to deliver near-real-time updates to fans. FotMob’s system has to handle extremely high volumes, with billions of push notifications per week and up to 1.6 million HTTP requests per second during peak hours. They previously used a third-party push notification system but couldn’t handle the load, so they built an in-house notification system on AWS, which is both reliable and saves over $130,000 per year.\nBlog 2 - Accelerating the next wave of generative AI startups AWS has announced a $230 million investment commitment to accelerate the wave of startups developing Generative AI applications globally. Amazon Web Services, Inc. Along with that, they reintroduced the AWS Generative AI Accelerator program — a 10-week (hybrid) program, in partnership with NVIDIA, to support potential startups: this year the program expanded fourfold, serving up to 80 global startups, each receiving up to $1 million in AWS credits to develop, scale, and innovate.\nBlog 3 - How climate tech startups are building foundation models with Amazon SageMaker HyperPod Climate-tech startups are leveraging Amazon SageMaker HyperPod to build specialized foundation models using large, multimodal environmental datasets from satellite imagery, climate data, to geological data to address problems such as carbon capture, sustainable material design, local weather and climate prediction, ecosystem conservation, and geological mapping. HyperPod provides a powerful AI infrastructure: automated provisioning of a GPU cluster with thousands of units, monitoring, fault-tolerance, checkpointing, and autoscaling making large-scale computations fast, reliable, and cost effective. Startups like Orbital Materials and Hum.AI have successfully developed their own models for carbon capture material design and ecosystem analysis clearly demonstrating how HyperPod is expanding the capabilities of environmental AI, turning complex data into sustainable solutions.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.3-bedrock-models/","title":"Activate Bedrock Models","tags":[],"description":"","content":"Before deploying the solution, you need to activate the necessary Amazon Bedrock models in your AWS account.\nSteps to Activate Models Search for Amazon Bedrock in AWS Console Access Model catalog from the left navigation menu Select the corresponding model names: Anthropic Claude 3.5 Sonnet Anthropic Claude 3 Sonnet Anthropic Claude 3 Haiku Cohere - Embed Multilingual v3 Select \u0026ldquo;Open in playground\u0026rdquo; and send a test message to activate each model Note: Make sure you activate all four models in the ap-southeast-1 (Singapore) region as the solution is deployed in this region.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Reflection Report: “AWS Cloud Mastery Series #3 – AWS Well-Architected Security Pillar Workshop” Event Objectives Share and reinforce core knowledge in the AWS Well-Architected Framework – Security Pillar.\nEquip a mindset and skills to build systems that are secure, resilient, and compliant with AWS best practices.\nDive into the 5 key security pillars:\nIdentity \u0026amp; Access Management (IAM) Detection \u0026amp; Continuous Monitoring Infrastructure Protection Data Protection Incident Response Introduce the AWS Cloud Clubs initiative and community activities that support students learning cloud at universities.\nSpeakers Le Vu Xuan An – AWS Cloud Club Captain HCMUTE Tran Duc Anh – AWS Cloud Club Captain SGU Tran Doan Cong Ly – AWS Cloud Club Captain PTIT Danh Hoang Hieu Nghi – AWS Cloud Club Captain HUFLIT Huynh Hoang Long – AWS Community Builder Dinh Le Hoang Anh – AWS Community Builder / Cloud Engineer Trainee Nguyen Tuan Thinh – Cloud Engineer Trainee Nguyen Do Thanh Dat – Cloud Engineer Trainee Van Hoang Kha – Cloud Security Engineer, AWS Community Builder Thinh Lam – FCJ Member Viet Nguyen – FCJ Member Mendel Grabski (Long) – Ex-Head of Security \u0026amp; DevOps, Cloud Security Solution Architect Trinh Truong – Platform Engineer at TymeX, AWS Community Builder (Together with members from the FCJ Team and the AWS community.)\nKey Highlights Introduction to AWS Cloud Club \u0026amp; Security Foundation The event started with:\nAn introduction to AWS Cloud Club:\nA platform that helps students develop cloud skills through real-world scenarios. Opportunities to receive mentorship from AWS experts and the community. Community building and networking among students from universities such as HCMUTE, SGU, PTIT, HUFLIT… Security Foundation – Core AWS Security Concepts:\nLeast Privilege – Zero Trust – Defense in Depth: three foundational principles for modern security systems. Shared Responsibility Model: AWS secures the cloud. Customers secure what runs in the cloud. Top Cloud Threats in Vietnam:\nPublic S3 / databases / Redis Exposed access keys Misconfigured IAM EC2 compromised with malware (crypto mining) Lack of logging or GuardDuty not enabled Key message: Security = culture, not a feature – security is a culture, not just an add-on feature.\nPillar 1 – Identity \u0026amp; Access Management (IAM) IAM is the core of the Security Pillar:\nApply Least Privilege aggressively. Remove root access keys after initial setup. Avoid using wildcard permissions (*). Modern IAM Practices IAM Users: almost no longer recommended → prioritize:\nRoles \u0026gt; Users SSO \u0026gt; Local credentials OIDC \u0026gt; Access Keys IAM Identity Center (SSO):\nManage users and permissions in a multi-account environment. Organizational controls:\nUse SCP (Service Control Policies) to limit permissions at the organization level. Use Permission Boundaries to restrict the maximum scope of what a user/role can do. Authentication \u0026amp; monitoring:\nEnforce credential rotation \u0026amp; MFA. Use IAM Access Analyzer to detect overly permissive, public, or unintended sharing policies. Compare MFA options TOTP vs FIDO2 in terms of security and recovery. Secrets Management:\nUse AWS Secrets Manager with the rotation lifecycle:\ncreate → set → test → finalize Do not hard-code secrets in code / environment variables. Pillar 2 – Detection \u0026amp; Continuous Monitoring Multi-layer Observability CloudTrail:\nManagement Events: API calls, configuration changes. Data Events: detailed activity on S3 objects, Lambda executions. Deploy organization-level CloudTrail across all accounts. Logging sources:\nVPC Flow Logs, S3 access logs, ALB logs to trace network and access patterns. Amazon GuardDuty:\nA continuous threat detection service that analyzes: CloudTrail events VPC Flow Logs DNS queries Detects: Logging being disabled Suspicious or abnormal traffic Access to malicious domains Malware scanning on S3 EKS audit logs RDS anomaly detection Lambda network behavior and runtime protection Mapped to AWS Foundational Security Best Practices \u0026amp; CIS Benchmarks. Security Hub:\nAggregates findings from GuardDuty, IAM Access Analyzer, Config, etc. into a central dashboard. Alerting \u0026amp; Automation Use EventBridge to:\nSend alerts (SNS, email, chat, …). Trigger Lambda / Step Functions for automatic remediation. Support cross-account event routing for centralized security operations. Detection-as-Code:\nManage detection logic as IaC: CloudTrail Lake queries Event patterns on EventBridge Enables controlled review, testing, and deployment of detection rules. Case study: If there are no logs → there is no evidence → you cannot investigate an incident.\nPillar 3 – Infrastructure Protection VPC Segmentation:\nReduce the blast radius when the system is compromised by segmenting and isolating environments. Subnet placement (Public vs Private):\nPublic: ALB, NAT Gateway, public bastion (if any). Private: EC2 app instances, databases, internal services. Security Group (SG) vs Network ACL (NACL):\nSG: stateful, attached to instances, the “primary firewall”. NACL: stateless, attached to subnets, an additional protection layer. Edge Protection:\nAWS WAF, Shield Advanced, AWS Network Firewall for edge defense. Integrate threat intelligence from GuardDuty for automated blocking. Workload Hardening:\nRegular patching for EC2/ECS/EKS. Minimize IAM permissions for workloads. Image scanning before deployment. Runtime protections. Case study: Many security issues come from placing resources in the wrong public subnet – always check placement before deployment.\nPillar 4 – Data Protection Encryption everywhere:\nEnable at-rest encryption for S3, EBS, RDS, DynamoDB. Enforce TLS/HTTPS when accessing S3, DynamoDB, RDS, etc. AWS KMS:\nManage CMK → Data Key. Key policies, Grants, rotation. Use IAM conditions to control encrypt/decrypt operations. Secrets Management:\nSecrets Manager / SSM Parameter Store: Automatic secret rotation. Separate config \u0026amp; secrets from application code. ACM (AWS Certificate Manager):\nProvides free certificates + auto-renew for ALB, CloudFront, API Gateway. Data classification \u0026amp; guardrails:\nClassify data by sensitivity (Public / Internal / Confidential / Restricted). Apply matching guardrails: IAM, encryption, network controls. Case study: Encrypting from the beginning significantly reduces risk in case of a data breach.\nPillar 5 – Incident Response (IR) AWS IR Lifecycle:\nPrepare → Detect → Investigate → Respond → Recover Playbooks (examples) Compromised IAM key Public S3 bucket EC2 infected with malware / crypto mining Tools \u0026amp; techniques Create Snapshots (EBS / instance) to preserve evidence. Isolation: modify SG / NACL to isolate compromised instances. Collect evidence for forensics \u0026amp; audit. IR Automation Lambda: run small remediation tasks (revert policy, block IPs, disable keys, …). Step Functions: orchestrate multi-step, complex IR workflows. EventBridge: trigger playbooks based on events from GuardDuty, CloudTrail, Security Hub… Conclusion: Incident Response cannot rely 100% on humans – as much as possible should be automated.\nKey Learnings Modern Security Mindset Zero Trust \u0026amp; Least Privilege: no implicit trust, every permission is minimized. Defense in Depth: multiple protective layers to reduce risk when one layer is bypassed. Traceability \u0026amp; IaC: Every change must be traceable. Infrastructure should be deployed using IaC to reduce misconfigurations. Do not trust manual configuration → favor automation + IaC + peer reviews. Key Technical Architecture IAM:\nRoles \u0026gt; Users SSO \u0026gt; Local credentials OIDC \u0026gt; Long-lived Access Keys Network:\nPrivate-first: by default, place resources in private subnets. SG as the “main wall” (stateful), NACL as a supporting layer (stateless). Outbound filtering is as important as inbound filtering. Data:\nEncrypt by default (S3, EBS, RDS, DynamoDB). Implement secret rotation according to policies. Minimize exposure for S3/DB. Detection:\nCloudTrail ON GuardDuty ON Sufficient logging to support incident investigations. Incident Response:\nClear playbooks. Automation for remediation and rollback. Snapshots/backups for fast recovery. Modernization Strategy Avoid “big bang” changes, follow a phased approach. Use multi-account architecture to reduce blast radius and separate environments. Applying to Work In the team’s AI Chatbot project:\nDesign IAM based on Roles + Permission Sets instead of direct users. Apply VPC segmentation and a private-first design for backend, databases, and vector stores. Enable GuardDuty + org-level CloudTrail across all dev/prod environments. Set up Alerting \u0026amp; IR automation: EventBridge → Lambda / Step Functions for auto-remediation. Use Secrets Manager + rotation instead of hard-coding secrets in code/ENV. Deploy infrastructure using Terraform/CDK to reduce drift and configuration errors. Expected outcome: a chatbot system that is more stable, more secure, easier to audit, and easier to scale.\nEvent Experience The event was well-organized and gave me a comprehensive view of the Security Pillar in the AWS Well-Architected Framework. I learned how AWS and large enterprises: Organize security using a multi-account model. Handle incident \u0026amp; detection in real-world production environments. Technical experience Practiced analyzing policies using IAM Simulator. Observed the real flow of S3 public exposure → auto-remediation. Saw IR automation in action for a compromised EC2 instance. Mindset \u0026amp; networking Understood how large organizations structure security around multi-account architectures. Internalized the mindset of “secure by design” – not waiting for an attack to start fixing things. Connected with peers passionate about security/cloud, creating a strong foundation for future learning and work. Takeaways Security = culture, not a feature. Misconfiguration is the biggest root cause → IaC is the lifesaver. You cannot operate safely in the cloud without: strong IAM + comprehensive logging + clear IR processes. Some photos from the event "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand how to launch and manage both Windows and Linux EC2 instances. Learn how to deploy applications on EC2 (Node.js \u0026amp; AWS User Management App). Understand IAM governance and cost usage governance in EC2 environments. Learn how applications securely access AWS services using IAM roles instead of access keys. Practice attaching IAM roles to EC2 and testing application access. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study EC2 introduction \u0026amp; preparation steps + EC2 concepts + Resources required + IAM \u0026amp; networking prerequisites 30/09/2025 30/09/2025 https://000004.awsstudygroup.com/ 2 - Launch EC2 instances: + Launch Windows Server 2022 instance + Launch Amazon Linux instance + Connect and validate OS-level access 01/10/2025 01/10/2025 https://000004.awsstudygroup.com/ 3 - Deploy applications on EC2: + Deploy AWS User Management App on Amazon Linux + Deploy Node.js application on Windows EC2 02/10/2025 02/10/2025 https://000004.awsstudygroup.com/ 4 - IAM Governance \u0026amp; Authorization: + Understand cost \u0026amp; usage governance with IAM + Compare Access Key vs IAM Role + Learn risks of long-term access keys 03/10/2025 03/10/2025 https://000048.awsstudygroup.com/ 5 - IAM Role Hands-on: + Create IAM Role for EC2 + Attach IAM Role to EC2 instance + Test application accessing AWS services via IAM Role + Clean up resources 04/10/2025 04/10/2025 https://000048.awsstudygroup.com/ Week 4 Achievements: Successfully launched both Windows and Linux EC2 instances. Understood how to prepare and configure an EC2 environment for application deployment. Deployed two applications: AWS User Management Application on Linux Node.js Application on Windows Learned about IAM governance and how permissions affect cost and security. Understood why Access Keys should not be used for applications. Created and attached IAM Roles to EC2 for secure access to AWS services. Verified application access using IAM Role-based credentials. Cleaned up all unused resources to avoid unnecessary costs. "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.4-aws-cli/","title":"Configure AWS CLI","tags":[],"description":"","content":"To deploy and manage the solution, you need to configure the AWS Command Line Interface (AWS CLI) with your authentication information.\nSteps Step 1: Check if AWS CLI is installed aws --version If not installed:\n# Download and install MSI installer msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Step 2: Create IAM User (if not already exists) Login to AWS Console Search \u0026ldquo;IAM\u0026rdquo; → Click IAM Left sidebar → Users → Create user User name: arc-workshop-user Click Next Attach policies directly, select policies: AmazonEC2FullAccess AmazonS3FullAccess AmazonDynamoDBFullAccess AmazonCognitoPowerUser AmazonSQSFullAccess AmazonTextractFullAccess AmazonBedrockFullAccess CloudWatchFullAccess IAMFullAccess Click Create user Step 3: Create Access Key Go to the created user → Tab Security credentials Scroll down Access keys → Click Create access key Select Command Line Interface (CLI) Tick \u0026ldquo;I understand\u0026hellip;\u0026rdquo; → Next Description: ARC Workshop CLI Click Create access key ⚠️ IMPORTANT: Copy or download .csv file\nAccess key ID: AKIAXXXXXXXXXXXXXXXX Secret access key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Step 4: Configure AWS CLI Open PowerShell and run:\naws configure Enter information:\nAWS Access Key ID [None]: AKIAXXXXXXXXXXXXXXXX AWS Secret Access Key [None]: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Default region name [None]: ap-southeast-1 Default output format [None]: json Step 5: Verify Configuration Check identity:\naws sts get-caller-identity Expected output:\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/arc-workshop-user\u0026#34; } "},{"uri":"https://cbthien.github.io/fcj-workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in 3 events with opportunities provided by AWS. Each event brought me new perspectives and motivation to seek and conquer new knowledge and challenges. Along with that came networking and interaction with AWS community members.\nEvent 1 Event Name: AWS Cloud Mastery Series #1 workshop (AI/ML/GenAI)\nDate \u0026amp; Time: 15/11/2025\nLocation: Bitexco Financial Tower, 26th Floor, 2 Hai Trieu, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Amazon SageMaker, Generative AI with Amazon Bedrock, Bedrock Agents, MLOps, CI/CD workflow for containers\nEvent 2 Event Name: AWS Cloud Mastery Series #2 workshop (DevOps)\nDate \u0026amp; Time: 17/11/2025\nLocation: Bitexco Financial Tower, 26th Floor, 2 Hai Trieu, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: Modern DevOps mindset, CI/CD Pipeline, Infrastructure as Code (IaC), Container Services (ECS/EKS/App Runner), Monitoring \u0026amp; Observability\nEvent 3 Event Name: AWS Cloud Mastery Series #3 workshop (Security Pillar)\nDate \u0026amp; Time: 29/11/2025\nLocation: Bitexco Financial Tower, 26th Floor, 2 Hai Trieu, District 1, Ho Chi Minh City\nRole: Attendee\nMain Content: AWS Well-Architected Framework – Security Pillar, IAM, Detection \u0026amp; Monitoring, Infrastructure Protection, Data Protection, Incident Response\n"},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn how to create and work with an AWS Cloud9 development environment. Practice using Cloud9 IDE features and AWS CLI inside Cloud9. Understand Amazon S3 fundamentals and how to host a static website on S3. Configure S3 bucket security settings including Public Access Block and object permissions. Implement S3 features such as Versioning, Object Movement, and Cross-Region Replication. Learn how to integrate S3 with CloudFront for accelerating static websites. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Introduction to AWS Cloud9 + Create a Cloud9 instance + Explore IDE layout and features 06/10/2025 06/10/2025 https://000049.awsstudygroup.com/ 2 - Cloud9 Hands-on: + Use terminal \u0026amp; built-in editor + Run AWS CLI inside Cloud9 + Practice basic CLI operations 07/10/2025 07/10/2025 https://000049.awsstudygroup.com/ 3 - Begin Amazon S3 workshop: + Learn S3 fundamentals + Enable static website hosting + Configure Public Access Block \u0026amp; public object permissions 08/10/2025 08/10/2025 https://000057.awsstudygroup.com/ 4 - Test S3 static website + Configure CloudFront to accelerate website performance + Enable Bucket Versioning + Move objects within S3 09/10/2025 09/10/2025 https://000057.awsstudygroup.com/ 5 - Configure S3 Cross-Region Replication (CRR) + Review requirements and IAM permissions + Set up replication rules - Clean up all S3 and Cloud9 resources 10/10/2025 10/10/2025 https://000057.awsstudygroup.com/ Week 5 Achievements: Successfully created and used an AWS Cloud9 development environment. Learned Cloud9 basic features including editor usage, terminal, shortcuts, and integrations. Practiced AWS CLI commands directly inside Cloud9. Understood S3 static website hosting and configured bucket settings for public content delivery. Tested S3 website accessibility and improved performance using CloudFront. Enabled S3 Bucket Versioning and practiced object movement within S3. Configured Cross-Region Replication and understood real-world use cases. Cleaned up all Cloud9 and S3 resources to avoid unnecessary costs. "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.5-data-preparation/","title":"Data Preparation","tags":[],"description":"","content":"Data Preparation Before creating AWS resources, you need to download sample data to test the system.\nStep 1: Download Sample Data Access ARC Sample Data Download the data to your computer Extract the file, which will create a folder named DATA Document Requirements Limit Value Format PDF (text-based or scanned) Max size 50 MB Max pages 500 pages Recommended 10-100 pages Prepare AWS Resources Step 2: Create S3 Bucket S3 Bucket is used to store uploaded PDF documents.\nSearch for S3 in AWS Console Click Create bucket Configure bucket: Bucket name: arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; (replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your AWS Account ID) AWS Region: Asia Pacific (Singapore) ap-southeast-1 Keep other settings as default Click Create bucket 💡 Tip: To get your AWS Account ID, run:\naws sts get-caller-identity --query Account --output text Or create using CLI:\naws s3 mb s3://arc-documents-$(aws sts get-caller-identity --query Account --output text) --region ap-southeast-1 Step 3: Create DynamoDB Table DynamoDB Table is used to store document metadata.\nSearch for DynamoDB in AWS Console Click Create table Configure table: Table name: arc-documents Partition key: doc_id (String) Sort key: sk (String) Table settings: Default settings Click Create table Or create using CLI:\naws dynamodb create-table \\ --table-name arc-documents \\ --attribute-definitions \\ AttributeName=doc_id,AttributeType=S \\ AttributeName=sk,AttributeType=S \\ --key-schema \\ AttributeName=doc_id,KeyType=HASH \\ AttributeName=sk,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 Step 4: Create SQS Queue SQS Queue is used for IDP pipeline to process documents.\nSearch for SQS in AWS Console Click Create queue Configure queue: Type: Standard Name: arc-document-queue Keep other settings as default Click Create queue Or create using CLI:\naws sqs create-queue --queue-name arc-document-queue --region ap-southeast-1 Step 5: Verify Resources Check that all resources have been created:\n# S3 Bucket aws s3 ls | grep arc-documents # DynamoDB Table aws dynamodb describe-table --table-name arc-documents --region ap-southeast-1 --query \u0026#34;Table.TableName\u0026#34; # SQS Queue aws sqs get-queue-url --queue-name arc-document-queue --region ap-southeast-1 Step 6: Upload Data to S3 Upload PDF files from the DATA folder downloaded in Step 1:\n# Upload all files from DATA folder aws s3 cp DATA/ s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ --recursive # Or upload individual file aws s3 cp DATA/sample-document.pdf s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ 💡 Tip: Replace \u0026lt;YOUR-ACCOUNT-ID\u0026gt; with your AWS Account ID\nVerify upload:\naws s3 ls s3://arc-documents-\u0026lt;YOUR-ACCOUNT-ID\u0026gt;/uploads/ Checklist Before proceeding, make sure:\nAWS CLI installed and configured Terraform installed Docker installed and running Node.js 18+ installed Python 3.11+ installed Git installed Repository cloned IAM user created with sufficient permissions Bedrock models approved (Claude + Cohere) S3 Bucket created DynamoDB Table created SQS Queue created Sample documents uploaded to S3 "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":"AI-POWERED ACADEMIC RESEARCH CHATBOT ON AWS Overview In this workshop, we will build ARC (Academic Research Chatbot) - an intelligent chatbot system running on the AWS Serverless platform. This solution leverages Generative AI and RAG (Retrieval-Augmented Generation) to support academic research, document queries, and answer questions flexibly.\nInstead of answering questions based on fixed scripts (rule-based), the system uses the Claude 3.5 Sonnet model to understand natural language, query data from the vector database, and respond to users accurately.\nWorkshop Objectives After completing this workshop, you will:\nUnderstand RAG architecture and how to apply it in practice Deploy a complete chatbot system on AWS Use Amazon Bedrock (Claude 3.5 Sonnet + Cohere Embed) Build an IDP pipeline with Amazon Textract Implement vector search with Qdrant Deploy infrastructure with Terraform Integrate authentication with Amazon Cognito Content Introduction Preparation Activate Bedrock Models Configure AWS CLI Data Preparation Deploy Infrastructure Setup Backend API Setup IDP Pipeline Setup Frontend Using Chatbot Using Admin Dashboard Clean up Resources "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn how to deploy and manage Amazon RDS MySQL in a secure VPC architecture. Understand how EC2 instances connect to RDS in private subnets. Deploy an application that interacts with RDS and practice backup/restore operations. Learn how to deploy and manage applications using Amazon Lightsail. Practice cost optimization techniques using snapshots, scaling, alarms, and resource cleanup. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study RDS overview \u0026amp; prerequisites + Supported engines + VPC design for RDS (public EC2 → private RDS) 13/10/2025 13/10/2025 https://000005.awsstudygroup.com/ 2 - Hands-on RDS: + Launch EC2 instance in public subnet + Create RDS MySQL instance in private subnet + Configure security groups for connectivity 14/10/2025 14/10/2025 https://000005.awsstudygroup.com/ 3 - Application Deployment: + Deploy application that queries the RDS database + Test CRUD operations through EC2 15/10/2025 15/10/2025 https://000005.awsstudygroup.com/ 4 - Lightsail Workshop – Part 1: + Deploy database on Lightsail + Deploy WordPress, Prestashop, and Akaunting instances 16/10/2025 16/10/2025 https://000045.awsstudygroup.com/ 5 - Lightsail Workshop – Part 2: + Configure application security + Create snapshots + Upgrade to larger instance + Create CloudWatch alarms + Resource cleanup 17/10/2025 17/10/2025 https://000045.awsstudygroup.com/ Week 6 Achievements: Understood RDS architecture and how relational databases operate within AWS VPC. Successfully deployed EC2 and RDS MySQL with proper networking and security groups. Deployed an application connected to RDS and validated database operations. Performed RDS snapshot backup \u0026amp; restore operations. Gained experience deploying applications on Amazon Lightsail (WordPress, Prestashop, Akaunting). Learned how to secure Lightsail applications using best practices. Practiced cost optimization through snapshots, scaling instances, and alarms. Completed full cleanup of Lightsail and RDS resources to avoid unnecessary charges. "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.6-infrastructure/","title":"Deploy Infrastructure","tags":[],"description":"","content":"In this section, we will clone the repository and deploy the entire AWS infrastructure for the ARC Chatbot system.\nStep 1: Clone Repository Clone the repository from GitHub:\ngit clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project Step 2: Build Dashboard Before deploying the application, we need to build the frontend dashboard.\nNavigate to the frontend folder cd frontend Install dependencies Run the following command to install the necessary libraries:\nnpm install Build Dashboard After installation is complete, run the build command:\nnpm run build After the process is complete, a dist folder will be created. Verify the index.html file and the assets folder:\nls dist/ # index.html assets/ Return to the project root directory cd .. Step 3: Deploy Terraform Application Deploy the Terraform application. The process will take approximately 20-30 minutes to deploy all resources.\ncd terraform terraform init terraform apply --auto-approve ⚠️ Note: If you encounter an error at this step, make sure Docker is running on your computer.\n💡 Info: Replace \u0026lt;account_id\u0026gt; with your actual AWS Account ID.\nStep 4: Verify Deployment After completing all the steps above, your environment has been successfully deployed.\nYou can verify the deployment by checking:\nAWS Console: Check the resources that have been created (EC2, S3, Cognito, DynamoDB, etc.) Terraform State: Run terraform state list to see the list of resources S3 Buckets: Buckets for documents and frontend have been created EC2 Instance: Instance for backend has been launched Check Outputs terraform output Important outputs:\nOutput Description api_endpoint Backend API URL cognito_user_pool_id Cognito User Pool ID cognito_client_id Cognito App Client ID s3_bucket_name S3 bucket for documents cloudfront_url Frontend URL Next Steps Now you can proceed to:\nSet up Backend Set up IDP Pipeline Set up Frontend "},{"uri":"https://cbthien.github.io/fcj-workshop/6-self-evaluation/","title":"Self-Evaluation","tags":[],"description":"","content":"During my internship at Amazon Web Services, Inc. from September 6, 2025 to December 9, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired at school to a real-world working environment.\nI participated in the Internal Learning Chatbot Project, through which I improved my skills in programming, analysis, and reporting. In terms of work ethic, I consistently tried to complete tasks effectively, follow company policies, and actively communicate with teammates to enhance work efficiency.\nTo objectively reflect on my internship experience, I hereby provide my self-evaluation based on the criteria below:\nNo. Criteria Description Good Fair Average 1 Knowledge and Professional Skills Understanding of the field, ability to apply knowledge in practice, tool usage, work quality ☐ ✅ ☐ 2 Ability to Learn Ability to acquire new knowledge, learn quickly ☐ ✅ ☐ 3 Proactiveness Willingness to explore tasks, take initiative without waiting for instructions ☐ ☐ ✅ 4 Sense of Responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Following working hours, company rules, and internal processes ✅ ☐ ☐ 6 Growth Mindset Willingness to accept feedback and improve oneself ☐ ✅ ☐ 7 Communication Ability to express ideas and report tasks clearly ☐ ✅ ☐ 8 Teamwork Ability to collaborate effectively with colleagues ✅ ☐ ☐ 9 Professional Conduct Respect for colleagues, partners, and the workplace ✅ ☐ ☐ 10 Problem-Solving Skills Ability to identify issues, propose solutions, and think creatively ☐ ✅ ☐ 11 Contribution to Project/Organization Work effectiveness, initiative contributions, recognition from the team ✅ ☐ ☐ 12 Overall Evaluation General assessment of the entire internship ☐ ✅ ☐ Areas for Improvement Develop better communication skills in daily interactions and professional situations. Strengthen discipline and strictly follow the policies and regulations of the company or any organization. Improve problem-solving thinking and analytical approaches. Areas for Improvement Category Improvement Opportunities Time Management - Optimize balancing between learning multiple AWS services and diving deeper into specific topics.\n- Improve time estimation for complex deployment tasks. Technical Communication - Enhance ability to explain complex architectures verbally to non-technical audiences.\n- Practice presenting technical trade-offs more concisely during time-limited meetings. Proactive Issue Prevention - Strengthen the ability to anticipate potential issues before they occur.\n- Improve pre-deployment checks to detect configuration errors earlier. Future Development Goals Goal Area Description AI/ML Expand expertise in SageMaker, Bedrock, and Comprehend to build advanced chatbot capabilities. DevOps Learn CI/CD pipelines, automated testing, and observability with CloudWatch and X-Ray. AWS Community Contribute to open-source AWS projects and share knowledge through technical blogs. Advanced Architecture Explore multi-region architectures and disaster recovery strategies. Conclusion The 12-week internship at First Cloud Journey provided invaluable hands-on experience with AWS and real-world project development. I have progressed from a beginner to someone capable of designing and deploying production-ready cloud solutions. These achievements form a solid foundation for my future career in cloud computing.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn how to deploy and optimize AWS architecture for a real project. Review core AWS fundamentals. Experiment with deploying a Text-to-SQL chatbot and evaluate extension directions. Analyze database security and data-processing logic in the system. Strengthen the ability to reason about cost, reliability, and security when evolving an existing architecture. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 2 - Study how to downscale the project. - Learn how to implement an architecture on AWS. - Note a rare incident: AWS outage in us-east-1, affecting many systems. 20/10/2025 20/10/2025 Internal AWS reports, incident reports 3 - Review core AWS knowledge to prepare for assessments. 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Redraw the overall architecture and estimate pricing for the project. - Define the initial direction for the project chatbot. - Direction document (Notion). 22/10/2025 22/10/2025 Draw.io 5 - Study database security and safe data logic to prevent invalid user operations (incorrect update/insert/delete). - Deploy the Text2SQL chatbot following the AWS Blog guide. - AWS Blog: Build an AI-powered Text-to-SQL Chatbot. 23/10/2025 23/10/2025 AWS Blog 6 - Read and understand the Text2SQL chatbot source code. - Meeting notes on Notion. - Team meeting: clarify how the chatbot interacts with the database, security measures, and DB access restrictions. - Propose replacing RAG with a DynamoDB cache to reduce costs compared to OpenSearch. 24/10/2025 24/10/2025 Notion, AWS Blog Week 7 Achievements Week 7 marked a transition from basic hands-on practice to working with a more realistic, production-oriented architecture. Instead of only following tutorials, I had to think about how the system should scale, how much it costs, and how to keep it safe while integrating a Text2SQL chatbot.\nImproved understanding of project downscaling and resource optimization\nAt the beginning of the week, I focused on how to downscale the existing project to match realistic constraints (student environment, limited budget, and controlled workload).\nReviewed which components truly needed to be running 24/7 and which could be scaled down or turned off when idle. Considered options like reducing instance sizes, using fewer AZs where acceptable, and removing non-essential test resources.\nThis helped me see that optimization is not always about “adding more services”, but often about removing or simplifying what is unnecessary. Reinforced AWS architectural foundation through review and application\nI revisited core AWS concepts (networking, compute, storage, IAM, security, fault tolerance) not only to prepare for assessments but also to map them directly to the current project:\nRe-checked how VPC, subnets, security groups, and routing fit together in the existing design. Linked theory from the study material to concrete architectural decisions in the chatbot system.\nThis review made my understanding more solid and less “exam-style”; I could better explain why certain AWS services were chosen in the architecture. Redesigned the overall architecture and produced a cost-aware version\nDuring the architecture redrawing task, I:\nCreated a new version of the architecture diagram that reflected the current project scope, including the Text2SQL chatbot, database, and supporting components. Added cost-related thinking into the diagram (for example: which components incur steady monthly cost, which scale with usage, and where free tiers or serverless options could help). Documented the architecture and pricing assumptions in Notion so that the team can discuss and challenge them later.\nThis was an important step in moving from “lab architecture” to something that resembles a real project design. Defined an initial direction for the Text2SQL chatbot\nI worked on clarifying how the chatbot should behave from both a user experience and system design perspective:\nDescribed the flow: from user question → model prompt → SQL generation → validation → execution → response formatting. Noted constraints such as maximum query complexity, timeout limits, and safety checks required before executing generated SQL. Wrote down open questions (e.g., how to log queries, how to monitor misuse, how to cap resource consumption).\nThis gave the project a clearer technical direction instead of just “we want a Text2SQL chatbot”. Deployed the Text2SQL chatbot by following the AWS Blog guide\nI followed the official AWS Blog tutorial to deploy the AI-powered Text-to-SQL chatbot:\nSet up the necessary infrastructure components (API, Lambda functions, database access, and Bedrock integration where applicable). Deployed the sample solution and verified that user questions were translated into SQL queries and executed correctly against the database. Checked logs and traces to ensure that failures (invalid queries, timeouts, or syntax errors) were handled gracefully.\nSuccessfully deploying the chatbot boosted my confidence in integrating AI components into cloud architectures. Analyzed database security and safe data-processing logic\nTo make sure the chatbot does not break the system or damage data, I:\nStudied safe data logic patterns to prevent dangerous operations such as unintended UPDATE/DELETE/INSERT without proper conditions. Considered using read-only database roles for Text2SQL queries, limiting modification capabilities. Thought about validation layers to inspect or restrict generated SQL before execution.\nThis helped me recognize that AI-driven systems need even stricter security and validation than traditional applications. Read and understood the Text2SQL chatbot source code\nI went through the source code provided in the solution:\nAnalyzed how prompts are constructed, how SQL is generated, and how responses are mapped back to user-friendly messages. Noted how the code handles error cases, logging, and integration with AWS services. Linked specific code modules to architecture components (e.g., which Lambda function is responsible for which part of the flow).\nThis gave me a deeper, implementation-level understanding instead of treating the solution as a “black box”. Evaluated replacing RAG with a DynamoDB-based cache to reduce costs\nIn the team meeting, we discussed the trade-offs between using RAG with OpenSearch and a DynamoDB cache:\nIdentified that for certain types of queries, a structured data store + cache could be cheaper and simpler than a full vector search solution. Proposed using DynamoDB as a cache for frequent queries or preprocessed results to reduce both latency and OpenSearch costs. Considered the impact on flexibility: DynamoDB cache is less “free-form” than RAG, but sufficient for well-defined query patterns.\nThis taught me how to reason about cost vs. flexibility and not default to over-engineering with more advanced services than necessary. Learned from a real AWS outage (us-east-1 incident)\nBy noting the AWS outage in us-east-1, I:\nReflected on how regional failures can impact many systems at once. Thought about how our architecture would behave in a similar scenario and what mitigation strategies (multi-AZ, multi-region, fallbacks) could be considered in the future.\nThis experience strengthened my awareness of resilience and fault tolerance beyond just single-region designs. 1. Self-assessment More confident discussing architecture and trade-offs\nI became more comfortable explaining why certain services are used, how they connect, and what trade-offs exist in terms of cost, performance, and complexity.\nBetter at linking code to architecture\nReading the Text2SQL chatbot source code and mapping it to the diagram helped me understand how high-level design decisions translate into actual implementation.\nImproved security awareness for AI-driven systems\nI now think more carefully about data access when LLMs or automated SQL generation are involved, instead of only checking whether “it works”.\n2. Challenges encountered Balancing complexity vs. cost\nIt was challenging to decide when advanced components (like RAG + OpenSearch) are justified and when a simpler pattern (like DynamoDB cache) is enough. This required more judgment than simply following best practices.\nReasoning about outages and resilience\nThinking about rare incidents such as regional outages is not intuitive at first. I had to stretch my thinking to consider “what if this entire region is not available?” instead of only focusing on normal operation.\nUnderstanding all details of the Text2SQL flow\nSome parts of the source code and the interaction between AI model, SQL generation, and database execution were complex. I needed time to trace the full request path and understand each step clearly.\nOverall, Week 7 helped me move closer to the mindset of a cloud engineer working on a real project: not just deploying components, but also optimizing architecture, controlling costs, protecting data, and planning for failures while integrating AI capabilities into the system.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/7-feedback/","title":"Feedback and Suggestions","tags":[],"description":"","content":"General Evaluation 1. Working Environment\nThe working environment at FCJ is comfortable and highly supportive. Team members are always willing to help whenever I face difficulties, even outside of working hours. The clean and organized workspace also makes it easier for me to stay focused during work.\n2. Support from Mentor / Team Admin\nMy mentor provided very dedicated guidance, always explaining clearly whenever I didn’t understand something and encouraging me to ask questions to broaden my thinking. The team admin also supported me with procedures and provided essential documents. What I appreciate most is that my mentor allowed me to explore and solve problems on my own instead of giving the answer immediately, helping me grow in problem-solving skills.\n3. Alignment Between Work and Academic Background\nThe tasks assigned to me were aligned with what I learned in my academic program, while also exposing me to new areas that I had not had the chance to explore before. This allowed me to strengthen my foundational knowledge while gaining practical skills relevant to the field.\n4. Learning Opportunities \u0026amp; Skill Development\nThroughout the internship, I gained many new skills such as using project management tools, collaborating within a team, and communicating in a professional environment. My mentor also shared valuable insights and real-world experience that helped me better define my career direction.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture at FCJ is very positive: everyone respects each other, works seriously but still maintains a cheerful atmosphere. The team collaborates closely and supports one another, which made me feel welcomed and truly part of the group, even as an intern.\n6. Policies / Benefits for Interns\nThe company provides internship allowances and offers flexible working hours when needed. Being able to participate in internal training sessions is also a big plus, helping me improve my skills and better understand the workplace culture.\n7. What did you like the most during your internship?\nWhat I appreciated the most was the friendly working environment and the dedicated support from my mentor. I always felt encouraged to try, make mistakes, and learn from them without pressure. This helped me gain more confidence in problem-solving and developing my professional skills.\n8. What do you think the company should improve for future interns?\nI believe the company could develop an updated onboarding document specifically for interns to help them get up to speed more quickly during the initial period.\n9. If you introduce this program to your friends, would you recommend they intern here? Why ?\nI would definitely recommend this internship to my friends. The environment here is ideal for those who want to learn while gaining real-world experience. The FCJ mentor team is very supportive and always creates opportunities for interns to grow instead of simply assigning tasks one-way.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.7-backend/","title":"Set up Backend API","tags":[],"description":"","content":"Set up Backend API In this section, you will configure the Backend API (FastAPI) and Qdrant vector database on EC2.\nBackend Architecture Internet → ALB (:80) → EC2 Private Subnet ├── FastAPI Container (:8000) ├── Qdrant Container (:6333) └── SQS Worker (background) 💡 Note: EC2 is located in a Private Subnet with no Public IP. Access via SSM Session Manager.\nStep 1: Access EC2 via Session Manager The EC2 instance was created in a Private Subnet with no Public IP. Use AWS Systems Manager Session Manager to access it.\nMethod 1: AWS Console Open AWS Console → EC2 → Instances Select instance arc-dev-app-server Click Connect → Session Manager → Connect Method 2: AWS CLI # Get Instance ID from Terraform output INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Connect via SSM aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 ⚠️ Requirement: Install Session Manager Plugin\nStep 2: Check Running Services EC2 has been automatically set up via user_data script when Terraform created the instance. Verify the services:\n# Switch to ec2-user sudo su - ec2-user # Check Docker containers docker ps You should see 2 containers running:\napp-fastapi-1 - FastAPI server (port 8000) app-qdrant-1 - Qdrant vector database (port 6333) # Check Qdrant curl http://localhost:6333/collections # Check FastAPI curl http://localhost:8000/health Step 3: Deploy Backend Code Backend code will be deployed via CI/CD Pipeline (CodePipeline → CodeBuild → CodeDeploy). However, for quick testing, you can deploy manually:\ncd /home/ec2-user # Clone repository git clone https://github.com/CrystalJohn/ARC-project.git cd ARC-project/backend # Stop old containers cd /home/ec2-user/app docker-compose down # Copy backend code cp -r /home/ec2-user/ARC-project/backend/* /home/ec2-user/app/ # Start with new code docker-compose up -d --build Step 4: Configure Environment Variables Create a .env file with values from Terraform outputs:\ncd /home/ec2-user/app # Get values from Terraform outputs (run on local machine) # terraform -chdir=terraform output cat \u0026gt; .env \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # AWS Configuration AWS_REGION=ap-southeast-1 # S3 S3_BUCKET_NAME=arc-documents-\u0026lt;account-id\u0026gt; # DynamoDB DYNAMODB_TABLE_NAME=arc-dev-documents # SQS SQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account-id\u0026gt;/arc-dev-document-queue # Qdrant (local container) QDRANT_HOST=qdrant QDRANT_PORT=6333 # Cognito COGNITO_USER_POOL_ID=ap-southeast-1_xxxxx COGNITO_CLIENT_ID=xxxxx # Bedrock BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0 EMBEDDING_MODEL_ID=amazon.titan-embed-text-v2:0 EOF 💡 Tip: Replace \u0026lt;account-id\u0026gt; and xxxxx values with actual outputs from Terraform.\nStep 5: Restart Services # Restart to load new .env docker-compose down docker-compose up -d # Check logs docker-compose logs -f fastapi Step 6: Verify via ALB Backend is exposed via Application Load Balancer. Verify from your local machine:\n# Get ALB DNS from Terraform output ALB_DNS=$(terraform -chdir=terraform output -raw alb_dns_name) # Test health endpoint curl http://$ALB_DNS/health # {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;} Step 7: Check Qdrant Collection # On EC2 curl http://localhost:6333/collections # Create collection for documents (if not exists) curl -X PUT http://localhost:6333/collections/documents \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;vectors\u0026#34;: { \u0026#34;size\u0026#34;: 1024, \u0026#34;distance\u0026#34;: \u0026#34;Cosine\u0026#34; } }\u0026#39; 💡 Note: Vector size 1024 corresponds to Amazon Titan Embeddings v2.\nChecklist Successfully accessed EC2 via Session Manager Docker containers running (fastapi, qdrant) .env file configured Health check via ALB successful Qdrant collection created Troubleshooting Unable to connect to Session Manager # Check SSM Agent on EC2 sudo systemctl status amazon-ssm-agent # Verify IAM Role has AmazonSSMManagedInstanceCore policy Container fails to start # View logs docker-compose logs # Check disk space df -h ALB health check fails # Verify Security Group allows port 8000 from ALB # Verify FastAPI is listening on 0.0.0.0:8000 docker-compose logs fastapi "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review AWS service knowledge to prepare for the midterm exam. Practice quizzes and sample exams to reinforce theory and practical skills. Tasks to carry out this week: Day Tasks Start date Completion date Reference Material 1 - Continue reviewing AWS services for the midterm - Try AI-generated practice tests to check knowledge 27/10/2025 27/10/2025 AI-generated quizzes, AWS Study Notes 2 - Continue reviewing AWS services for the midterm 28/10/2025 28/10/2025 Quizlet / AWS Docs 3 - Take sample quizzes to consolidate knowledge 29/10/2025 29/10/2025 AWS Quiz Practice 4 - Review theory via Quizlet: AWS Hotfix V10 - Try practice exam: AWS Practitioner Practice Exam - Try practice exam: AWS SAA-C03 - Review with video: AWS Solutions Architect Associate Course 30/10/2025 30/10/2025 Quizlet / GitHub / YouTube 5 - Midterm exam 31/10/2025 31/10/2025 AWS FCJ Midterm Test Week 8 Achievements: Reviewed all AWS service groups: Compute, Storage, Networking, Database, Security, Monitoring, etc. Reinforced knowledge through quizzes, mock exams, and practice questions. Gained solid understanding of core concepts for the AWS Practitioner and SAA-C03 exams. Became familiar with real exam structure and question formats. Completed the First Cloud Journey midterm. Improved ability to select appropriate AWS services for specific scenarios. "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.8-idp-pipeline/","title":"Set up IDP Pipeline","tags":[],"description":"","content":"Set up IDP Pipeline In this section, you will setup SQS Worker to process documents through IDP (Intelligent Document Processing) pipeline.\nIDP Flow Upload → S3 → DynamoDB (UPLOADED) → SQS ↓ EC2 Worker ↓ PyPDF2 (digital) / Textract (scanned) ↓ Chunk Text (1000 tokens) ↓ Cohere Embed Multilingual v3 (Bedrock) ↓ Qdrant (store vectors) ↓ DynamoDB (EMBEDDING_DONE) 💡 Note: Worker uses PyPDF2 for digital PDF (text-based) and Textract for scanned PDF (image-based).\nDocument States Status Description UPLOADED File uploaded, waiting for processing IDP_RUNNING Worker is processing TEXTRACT_DONE OCR completed (scanned PDF only) EMBEDDING_DONE Completed, ready to use FAILED Error occurred Step 1: Access EC2 via Session Manager # Get Instance ID INSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) # Connect aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 After connecting:\nsudo su - ec2-user cd /home/ec2-user/backend 💡 Note: EC2 has 2 folders:\napp/ - Boilerplate from user_data script backend/ - Actual code deployed via CI/CD (contains run_worker.py) Step 2: Check Worker Code Worker code is in backend/run_worker.py. Verify the file exists:\nls -la # Must have: run_worker.py, app/, requirements.txt Step 3: Configure Environment Ensure .env file has all variables (in backend/ folder):\ncd /home/ec2-user/backend cat .env Important variables for IDP:\nSQS_QUEUE_URL=https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue S3_BUCKET=arc-documents-\u0026lt;account\u0026gt; QDRANT_HOST=localhost QDRANT_PORT=6333 AWS_REGION=ap-southeast-1 Step 4: Start Worker Option A: Run directly (for debugging) # Activate virtual environment (if available) source venv/bin/activate # Run worker python run_worker.py Worker will display:\n============================================================ IDP Pipeline - SQS Worker ============================================================ Queue URL: https://sqs.ap-southeast-1.amazonaws.com/xxx/arc-dev-document-queue Bucket: arc-documents-xxx Region: ap-southeast-1 Qdrant: localhost:6333 ------------------------------------------------------------ Processing indefinitely (Ctrl+C to stop)... Option B: Run in background with nohup nohup python run_worker.py \u0026gt; worker.log 2\u0026gt;\u0026amp;1 \u0026amp; # Check process ps aux | grep run_worker # View logs tail -f worker.log Option C: Run in Docker (recommended) # Add worker to docker-compose.yml docker-compose up -d worker Step 5: Test IDP Pipeline 5.1 Upload test file to S3 # From local machine aws s3 cp test-sample.pdf s3://arc-documents-\u0026lt;account\u0026gt;/uploads/test-001.pdf 5.2 Create record in DynamoDB aws dynamodb put-item \\ --table-name arc-dev-documents \\ --item \u0026#39;{ \u0026#34;doc_id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-001\u0026#34;}, \u0026#34;sk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;METADATA\u0026#34;}, \u0026#34;filename\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test-sample.pdf\u0026#34;}, \u0026#34;s3_key\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;}, \u0026#34;status\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;UPLOADED\u0026#34;}, \u0026#34;uploaded_at\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;\u0026#39;$(date -u +%Y-%m-%dT%H:%M:%SZ)\u0026#39;\u0026#34;} }\u0026#39; 5.3 Send message to SQS aws sqs send-message \\ --queue-url https://sqs.ap-southeast-1.amazonaws.com/\u0026lt;account\u0026gt;/arc-dev-document-queue \\ --message-body \u0026#39;{ \u0026#34;doc_id\u0026#34;: \u0026#34;test-001\u0026#34;, \u0026#34;s3_key\u0026#34;: \u0026#34;uploads/test-001.pdf\u0026#34;, \u0026#34;filename\u0026#34;: \u0026#34;test-sample.pdf\u0026#34; }\u0026#39; Step 6: Monitor Processing View worker logs:\n# If running directly # Logs displayed on terminal # If running in background tail -f worker.log Successful logs will look like:\n2024-01-15 10:30:00 - INFO - Received message for doc_id: test-001 2024-01-15 10:30:01 - INFO - Downloading from S3: uploads/test-001.pdf 2024-01-15 10:30:02 - INFO - Extracting text with PyPDF2... 2024-01-15 10:30:03 - INFO - Created 8 chunks from document 2024-01-15 10:30:05 - INFO - Generating embeddings with Cohere... 2024-01-15 10:30:10 - INFO - Stored 8 vectors for test-001 2024-01-15 10:30:10 - INFO - Updated status: EMBEDDING_DONE 2024-01-15 10:30:10 - INFO - Document test-001 processed successfully Step 7: Verify Processing 7.1 Check DynamoDB aws dynamodb get-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --query \u0026#39;Item.{status:status.S,chunks:chunk_count.N}\u0026#39; Expected output:\n{ \u0026#34;status\u0026#34;: \u0026#34;EMBEDDING_DONE\u0026#34;, \u0026#34;chunks\u0026#34;: \u0026#34;8\u0026#34; } 7.2 Check Qdrant # On EC2 curl -s http://localhost:6333/collections/arc_documents/points/count | jq Expected output:\n{ \u0026#34;result\u0026#34;: { \u0026#34;count\u0026#34;: 8 } } Error Handling Issue Cause Solution Worker not receiving messages SQS URL incorrect Check .env Bedrock timeout Rate limit Increase retry delay Qdrant connection refused Container not started docker-compose up -d qdrant FAILED status Check error_message in DynamoDB Fix and retry Retry Failed Document # Update status back to UPLOADED to retry aws dynamodb update-item \\ --table-name arc-dev-documents \\ --key \u0026#39;{\u0026#34;doc_id\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;test-001\u0026#34;},\u0026#34;sk\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;METADATA\u0026#34;}}\u0026#39; \\ --update-expression \u0026#34;SET #s = :s\u0026#34; \\ --expression-attribute-names \u0026#39;{\u0026#34;#s\u0026#34;:\u0026#34;status\u0026#34;}\u0026#39; \\ --expression-attribute-values \u0026#39;{\u0026#34;:s\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;UPLOADED\u0026#34;}}\u0026#39; # Send message to SQS again aws sqs send-message \\ --queue-url $SQS_QUEUE_URL \\ --message-body \u0026#39;{\u0026#34;doc_id\u0026#34;:\u0026#34;test-001\u0026#34;,\u0026#34;s3_key\u0026#34;:\u0026#34;uploads/test-001.pdf\u0026#34;,\u0026#34;filename\u0026#34;:\u0026#34;test-sample.pdf\u0026#34;}\u0026#39; Checklist Access EC2 via Session Manager Worker code is available Environment variables configured Worker is running Test document uploaded to S3 SQS message sent Worker processed document (logs) Status = EMBEDDING_DONE in DynamoDB Vectors stored in Qdrant "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.9-frontend/","title":"Setup Frontend","tags":[],"description":"","content":"Setup Frontend Configure and deploy Frontend React application with AWS Amplify.\nStep 1: Get Terraform Outputs cd terraform terraform output Note: cognito_user_pool_id, cognito_client_id, alb_dns_name\nStep 2: Configure Environment cd ARC-project cp .env.example .env Edit .env:\nVITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_POOL_ID=ap-southeast-1_xxxxxxx VITE_COGNITO_CLIENT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxx VITE_API_URL=http://arc-chatbot-dev-alb-xxxxx.ap-southeast-1.elb.amazonaws.com Step 3: Install \u0026amp; Test Local npm install npm run dev Open http://localhost:5173\nStep 4: Build \u0026amp; Deploy npm run build Push code to GitHub, Amplify will deploy automatically:\ngit add . git commit -m \u0026#34;Update frontend config\u0026#34; git push origin main 💡 Amplify app was created via Terraform and connected with GitHub.\nStep 5: Update Cognito Callback URLs After getting Amplify URL:\naws cognito-idp update-user-pool-client \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --client-id xxxxxxxxxx \\ --callback-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; \\ --logout-urls \u0026#34;http://localhost:5173\u0026#34; \u0026#34;https://main.xxxxx.amplifyapp.com\u0026#34; Step 6: Create Test Users # Admin user aws cognito-idp admin-create-user \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --user-attributes Name=email,Value=admin@example.com \\ --temporary-password \u0026#34;TempPass123!\u0026#34; aws cognito-idp admin-add-user-to-group \\ --user-pool-id ap-southeast-1_xxxxxxx \\ --username admin@example.com \\ --group-name admin Error Handling Error Solution CORS error Check FastAPI CORS config \u0026ldquo;User pool does not exist\u0026rdquo; Check VITE_COGNITO_POOL_ID Build failed Check Amplify environment variables Checklist .env configured Local dev server running Amplify deploy successful Cognito callback URLs updated Login/Register working "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn how to deploy a scalable and highly available application using Launch Templates, Load Balancers, and Auto Scaling Groups. Understand elasticity, fault tolerance, and load distribution in AWS. Learn how to monitor AWS resources using CloudWatch Metrics, Logs, Alarms, and Dashboards. Implement observability best practices for application monitoring and troubleshooting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study Auto Scaling architecture + Review requirements and base AMI + Prepare IAM roles, security groups, and networking 03/11/2025 03/11/2025 https://000006.awsstudygroup.com/ 2 - Hands-on Part 1: + Create Launch Template + Configure instance settings, user data, and storage 04/11/2025 04/11/2025 https://000006.awsstudygroup.com/ 3 - Hands-on Part 2: + Set up Application Load Balancer + Register target groups + Test application availability 05/11/2025 05/11/2025 https://000006.awsstudygroup.com/ 4 - Auto Scaling Group Deployment: + Create ASG with scaling policies + Test scaling behavior and failover scenarios 06/11/2025 06/11/2025 https://000006.awsstudygroup.com/ 5 - CloudWatch Workshop: + Explore CloudWatch Metrics + Configure CloudWatch Logs + Create Alarms + Build dashboards + Cleanup resources 07/11/2025 07/11/2025 https://000008.awsstudygroup.com/ Week 9 Achievements: Understood how Launch Templates, Load Balancers, and Auto Scaling Groups work together to provide fault tolerance and scalability. Created a fully functional ASG environment and tested scaling policies. Successfully deployed and tested an ALB with registered targets. Learned how to capture logs and metrics for EC2 and applications. Created CloudWatch alarms to monitor resource health and performance. Built CloudWatch dashboards for visual monitoring. Completed full cleanup of Auto Scaling, Load Balancer, and CloudWatch resources. "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.10-using-chatbot/","title":"Using Chatbot","tags":[],"description":"","content":"Guide to using ARC Chatbot to search for information from research documents.\nAccess Local: http://localhost:5173 Production: Amplify URL from previous step Step 1: Login Enter email and password Click Login Redirect to Chat page Step 2: Chat Interface After logging in, you will see:\nSidebar with Chat, History menus Header with user info and dark mode toggle Chat area with welcome message Step 3: Ask Questions Enter a question in the input box and press Enter or click Send.\nGood Questions Type Example Definition \u0026ldquo;What is a stack data structure?\u0026rdquo; Comparison \u0026ldquo;Compare stack and queue\u0026rdquo; Explanation \u0026ldquo;Explain binary search algorithm\u0026rdquo; Avoid ❌ Too general: \u0026ldquo;Tell me about programming\u0026rdquo; ❌ Outside documents: \u0026ldquo;What\u0026rsquo;s the weather today?\u0026rdquo; Step 4: Citations Each answer has citations showing document sources:\n📚 Sources: [1] data-structures.pdf - Page 12 - Score: 85% [2] algorithms.pdf - Page 45 - Score: 72% Click on citation to view document details.\nField Description [1], [2] Citation number Filename PDF filename Page Page number Score Relevance (%) Step 5: Conversation History Click History in sidebar View list of previous conversations Click conversation to reload it Click trash icon to delete Step 6: New Chat Click New Chat in sidebar to start a new conversation.\nFeatures Feature Description Streaming Response displays in parts Markdown Supports code blocks, lists, headers Dark Mode Toggle in header History Save and reload conversations Checklist Successfully logged in Sent query and received response Citations displayed correctly Click citation to view document History working New Chat working "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives Learn how to install, configure, and use the AWS CLI to interact with AWS services. Understand how to manage AWS resources (S3, SNS, IAM, VPC, EC2) through AWS CLI commands. Explore Route 53 Resolver and build a hybrid DNS architecture between on-premises and AWS. Deploy Microsoft AD and configure DNS forwarding using Route 53 Resolver. Tasks to be carried out this week Day Task Start Date Completion Date Reference 1 - Introduction to AWS CLI - Supported command environments (Linux/macOS/Windows/Remote) - Create and understand CLI profiles - Configure AWS CLI using aws configure 10/11/2025 10/11/2025 https://000011.awsstudygroup.com/ 2 - Install AWS CLI - View AWS resources using CLI - Interact with Amazon S3 via CLI (list buckets, upload/download objects) 11/11/2025 11/11/2025 https://000011.awsstudygroup.com/ 3 - Use AWS CLI with Amazon SNS (create topic, subscription, publish) - Use AWS CLI with IAM (create users, list policies) - Use AWS CLI with VPC (list VPCs, subnets, SGs) 12/11/2025 12/11/2025 https://000011.awsstudygroup.com/ 4 - Create EC2 instance using AWS CLI - Troubleshoot CLI errors - Clean up CLI resources Start Route 53 Hybrid DNS Lab: - Introduction to Hybrid DNS with Route 53 Resolver 13/11/2025 13/11/2025 https://000011.awsstudygroup.com/ https://000010.awsstudygroup.com/ 5 - Deploy Microsoft AD (in lab environment) - Configure Route 53 Resolver rules - Setup DNS inbound \u0026amp; outbound endpoints - Test DNS resolution between on-premises and AWS - Clean up resources 14/11/2025 14/11/2025 https://000010.awsstudygroup.com/ Week 10 Achievements Successfully installed AWS CLI and configured profiles using Access Key, Secret Key, and Region. Viewed AWS resources and interacted with S3, SNS, IAM, and VPC using CLI commands. Launched an EC2 instance using CLI and resolved configuration/debugging issues. Gained hands-on experience with Route 53 Resolver and hybrid DNS architecture. Deployed Microsoft Active Directory and configured DNS forwarding between on-premises and AWS. Validated DNS resolver rules, inbound/outbound endpoints, and confirmed hybrid DNS functionality. Cleaned up all resources to avoid unnecessary charges. "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"WEEK 11 WORKLOG Week 11 Objectives Complete the base infrastructure (M0) including S3, DynamoDB, Cognito, ALB. Begin implementing the IDP Pipeline (M1) — SQS, PDF Detection, PyPDF2 extraction. Stabilize and optimize the previous Analytics pipeline: migrate from Glue Crawler to Athena DDL to reduce cost and simplify the architecture. Standardize Lambda functions (AdminManager, Analytics Lambda) and ensure stable interactions with both Athena and RDS. Optimize Cognito configuration to decouple backend logic from authentication. Test the full end-to-end pipeline from document upload → detection → extraction → DynamoDB → frontend. Tasks to Implement This Week Day Tasks Start Date Completion Date Documentation 2 - Set up S3 bucket arc-chatbot-documents-427995028618 - Create DynamoDB tables (metadata, chat-history) - Configure schema \u0026amp; indexes 17/11/2025 17/11/2025 https://docs.aws.amazon.com/dynamodb/ 3 - Configure Cognito User Pool ap-southeast-1_8KB4JYvsX - Set up user authentication flow - Test sign-up/sign-in process 18/11/2025 18/11/2025 https://docs.aws.amazon.com/cognito/ 4 - Set up ALB arc-chatbot-dev-alb with health checks - Configure target groups \u0026amp; listeners - Test routing 19/11/2025 19/11/2025 https://docs.aws.amazon.com/elasticloadbalancing/ 5 - Create SQS queue arc-chatbot-dev-document-processing - Implement PDF detection (digital vs scanned) - Write unit tests for PDF detector 20/11/2025 20/11/2025 https://docs.aws.amazon.com/sqs/ 6 - Implement PyPDF2 extraction for digital PDFs - Handle edge cases (encrypted, corrupted) - Write tests (30 tests passed) 21/11/2025 21/11/2025 https://pypdf2.readthedocs.io/ Week 11 Achievements 1. Completed Base Infrastructure (M0) S3 bucket arc-chatbot-documents-427995028618 created for document storage. DynamoDB tables: metadata: stores document metadata chat-history: stores conversation logs Cognito User Pool ap-southeast-1_8KB4JYvsX — used for authentication \u0026amp; authorization. ALB arc-chatbot-dev-alb — handles API traffic routing with health checks. 2. Initiated \u0026amp; Optimized the IDP Pipeline (M1) SQS Queue arc-chatbot-dev-document-processing created for document processing flows. PDF Detector service implemented: detects digital vs scanned PDFs (17 tests passed). PDF Extractor (PyPDF2): extracts text from digital PDFs (30 tests passed). Added handling for problematic PDFs: encrypted, corrupted, unsupported. 3. Stabilized \u0026amp; Improved the Analytics Pipeline (from previous week) Removed Glue Crawler → migrated to Athena DDL for better schema control. Deleted Glue DB and Glue endpoint → reduced cost \u0026amp; simplified architecture. Refactored Analytics Lambda to run outside VPC for Athena queries. AdminManager Lambda now focuses solely on inserting data into RDS → simpler architecture. Improved Cognito configuration to ensure the backend is no longer dependent on the DB. 4. Frontend \u0026amp; Authentication Progress Successfully deployed login page on CloudFront. Completed admin_handler logic to allow stable data insertion into RDS through the frontend. Lessons Learned Designing DynamoDB schema for a RAG/Chatbot system. Understanding Cognito authentication flow, JWT tokens, and User Pool mechanics. SQS message processing patterns \u0026amp; building an IDP pipeline. Techniques for reading PDFs with PyPDF2 and handling various error cases. Athena DDL architecture, differences from Glue Crawler, cost/performance best practices. Optimizing Lambda for a serverless data-processing pipeline. Summary Week 11 successfully achieved key objectives: completed M0, initiated M1, stabilized the analytics pipeline, and standardized Lambda and Cognito authentication flows. These improvements establish a strong foundation for continuing the RAG pipeline and expanding document-processing modules in the following weeks.\n"},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.11-admin-dashboard/","title":"Using Admin Dashboard","tags":[],"description":"","content":"Using Admin Dashboard Guide to using Admin Dashboard to manage documents.\nAccess URL: http://localhost:5173/admin (or Amplify URL) Requirement: Account in admin group Step 1: Login Admin Log in with admin account (created in previous step).\nStep 2: Dashboard Overview After logging in, you will see:\nUpload section (drag \u0026amp; drop) Documents table with pagination Status filter and auto-refresh Step 3: Upload Documents 3.1. Select files Drag \u0026amp; drop PDF files to upload area Or click Browse Files to select 3.2. Upload Progress Each file displays progress bar and status:\nuploading - Uploading success - Upload successful error - Upload failed Step 4: Document Status After upload, document will be processed through IDP pipeline:\nStatus Description Time UPLOADED Waiting for processing - IDP_RUNNING Processing 1-5 min EMBEDDING_DONE Ready - FAILED Error - 💡 Tip: Enable Auto-refresh (5s) to automatically update status.\nStep 5: Manage Documents Filter by Status Use Status dropdown to filter:\nAll Uploaded Processing Done Failed Pagination Documents are paginated (5 items/page). Use pagination controls at footer.\nView Document Click 👁️ icon to view document details.\nDelete Document Click 🗑️ icon to delete document.\n⚠️ Warning: Deleting document will delete from S3, DynamoDB, and Qdrant.\nStep 6: Processing History Click Processing History link to view document processing history.\nError Handling Issue Solution Upload failed Check file size (\u0026lt;50MB), format (PDF only) Document stuck in IDP_RUNNING Check worker logs on EC2 Document FAILED See error message in Processing History Checklist Logged in to admin dashboard Upload document successful Document processed (EMBEDDING_DONE) Filter/pagination working Auto-refresh working "},{"uri":"https://cbthien.github.io/fcj-workshop/5-workshop/5.12-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Cleanup Resources After completing the workshop, clean up AWS resources to avoid incurring charges.\n⚠️ Warning: These steps will PERMANENTLY DELETE all data and resources!\nCleanup Order Stop services on EC2 Empty S3 buckets Terraform destroy Verify cleanup Step 1: Stop Services on EC2 Connect EC2 via Session Manager:\nINSTANCE_ID=$(terraform -chdir=terraform output -raw ec2_instance_id) aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 Stop Docker containers:\nsudo su - ec2-user cd /home/ec2-user/app # Stop containers docker-compose down # Remove volumes docker volume rm app_qdrant_storage Step 2: Empty S3 Buckets S3 buckets must be empty before Terraform destroy:\n# Get bucket name from Terraform output BUCKET=$(terraform -chdir=terraform output -raw s3_bucket_name) # Empty bucket aws s3 rm s3://$BUCKET --recursive # Or force delete aws s3 rb s3://$BUCKET --force Step 3: Terraform Destroy cd terraform terraform plan -destroy terraform destroy Enter yes when prompted. This process takes about 10-15 minutes.\nStep 4: Manual Cleanup (if needed) If there are still resources not deleted:\n# CloudWatch Log Groups aws logs describe-log-groups --log-group-name-prefix /aws/arc | jq -r \u0026#39;.logGroups[].logGroupName\u0026#39; | xargs -I {} aws logs delete-log-group --log-group-name {} # EC2 Key Pair (if created manually) aws ec2 delete-key-pair --key-name arc-keypair # Amplify App (if created manually) aws amplify list-apps | jq -r \u0026#39;.apps[] | select(.name | contains(\u0026#34;arc\u0026#34;)) | .appId\u0026#39; | xargs -I {} aws amplify delete-app --app-id {} Step 5: Verify Cleanup # Check EC2 aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=*arc*\u0026#34; --query \u0026#39;Reservations[].Instances[].InstanceId\u0026#39; # Check S3 aws s3 ls | grep arc # Check RDS/DynamoDB aws dynamodb list-tables --query \u0026#39;TableNames[?contains(@, `arc`)]\u0026#39; # Check Lambda aws lambda list-functions --query \u0026#39;Functions[?contains(FunctionName, `arc`)].FunctionName\u0026#39; # Check ECR aws ecr describe-repositories --query \u0026#39;repositories[?contains(repositoryName, `arc`)].repositoryName\u0026#39; Expected output: All empty.\nCost Estimation Before cleanup, check estimated costs:\n# CloudWatch - Check billing dashboard for actual charges # https://console.aws.amazon.com/billing/ Checklist EC2 services stopped S3 buckets emptied Terraform destroy completed Manual cleanup verified All resources deleted Billing dashboard checked "},{"uri":"https://cbthien.github.io/fcj-workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives Complete the RAG Chat Component (M2) — Rate limiting, fallback model, error handling, and retry logic. Begin Testing \u0026amp; Go-live (M3) — Login page, Chat UI, Admin Dashboard, and initiate end-to-end testing flow. Tasks to carry out this week: Day Tasks Start Date Completion Date Reference 2 - Implement Rate Limiter (60 RPM, 100K TPM) - Add Budget Manager ($10/day, $200/month limits) - Write tests for rate-limiting logic 24/11/2025 24/11/2025 https://docs.aws.amazon.com/bedrock/ 3 - Implement fallback to Claude Haiku when Sonnet is rate-limited - Add error handling \u0026amp; retry logic with exponential backoff + jitter - Write tests (35 tests passed) 25/11/2025 25/11/2025 https://docs.aws.amazon.com/bedrock/ 4 - Implement login page integrated with Cognito Hosted UI - Build AuthService for JWT parsing \u0026amp; validation - Test full authentication flow end-to-end 26/11/2025 26/11/2025 https://docs.aws.amazon.com/cognito/ 5 - Build Chat UI using React - Implement ChatPage with message bubbles \u0026amp; citation display - Create CitationCard and DocumentViewerModal components 27/11/2025 27/11/2025 https://react.dev/ 6 - Build Admin Dashboard with drag-and-drop upload - Display real-time processing status (auto-refresh every 5s) - Begin E2E Integration Testing (Auth → Chat → Admin → Logs) 28/11/2025 28/11/2025 https://react.dev/ Week 12 Achievements 1. Completed RAG Chat (M2) Rate Limiter implemented for LLM requests: 60 requests/minute 100K tokens/minute Budget Manager added to guard against overspending: $10/day, $200/month caps 22 automated tests passed Retry mechanism implemented using exponential backoff + jitter for higher stability under transient API failures. Fallback model logic: Primary: Claude 3.5 Sonnet Automatic fallback: Claude Haiku during rate limit or timeout\n→ Significantly improves reliability and user experience. 2. Initiated Testing \u0026amp; Go-live (M3) AuthService with JWT validation fully implemented: Token parsing, signature validation, claim extraction Integrated into frontend login flow Chat UI v1.0 completed: Smooth message bubbles with support for streaming responses Inline citations \u0026amp; detailed citation modal New components: CitationCard — displays document reference DocumentViewerModal — previews processed PDF content Admin Dashboard: Drag-and-drop upload interface Document list \u0026amp; real-time status updates Auto-refresh for monitoring processing pipeline E2E Testing started: Auth → Chat interactions → Document upload \u0026amp; processing → Data displayed in UI. Learnings Rate-limiting patterns for AI APIs: token bucket, request throttling, and token-based limits. Cost-management patterns for Bedrock: fallback models, usage budgeting, token accounting. How to implement exponential backoff with jitter to reduce collision \u0026amp; improve reliability. Building advanced React UI components for chat systems (streaming messages, citation linking, modals). Cognito JWT validation — decoding tokens, verifying signatures, validating claims and expiry. "},{"uri":"https://cbthien.github.io/fcj-workshop/categories/best-practices/","title":"Best Practices","tags":[],"description":"","content":""},{"uri":"https://cbthien.github.io/fcj-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://cbthien.github.io/fcj-workshop/categories/responsible-ai/","title":"Responsible AI","tags":[],"description":"","content":""},{"uri":"https://cbthien.github.io/fcj-workshop/categories/startup/","title":"Startup","tags":[],"description":"","content":""},{"uri":"https://cbthien.github.io/fcj-workshop/categories/sustainability/","title":"Sustainability","tags":[],"description":"","content":""},{"uri":"https://cbthien.github.io/fcj-workshop/categories/aws-for-games-blog/","title":"AWS for Games Blog","tags":[],"description":"","content":""},{"uri":"https://cbthien.github.io/fcj-workshop/categories/aws-for-startups-blog/","title":"AWS for Startups Blog","tags":[],"description":"","content":""},{"uri":"https://cbthien.github.io/fcj-workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]